{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "XNiqq_kN0okj",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/conda/lib/python3.7/site-packages (21.1.2)\n",
      "Collecting tfx[kfp]==0.30.0\n",
      "  Downloading tfx-0.30.0-py3-none-any.whl (2.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.4 MB 1.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting ml-metadata<0.31,>=0.30\n",
      "  Downloading ml_metadata-0.30.0-cp37-cp37m-manylinux2010_x86_64.whl (2.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.9 MB 15.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy<1.20,>=1.16 in /opt/conda/lib/python3.7/site-packages (from tfx[kfp]==0.30.0) (1.19.5)\n",
      "Requirement already satisfied: protobuf<4,>=3.12.2 in /opt/conda/lib/python3.7/site-packages (from tfx[kfp]==0.30.0) (3.16.0)\n",
      "Collecting tfx-bsl<0.31,>=0.30\n",
      "  Downloading tfx_bsl-0.30.0-cp37-cp37m-manylinux2010_x86_64.whl (2.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.2 MB 25.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting kubernetes<12,>=10.0.1\n",
      "  Downloading kubernetes-11.0.0-py3-none-any.whl (1.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.5 MB 31.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-cloud-aiplatform<0.8,>=0.5.0\n",
      "  Downloading google_cloud_aiplatform-0.7.1-py2.py3-none-any.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 43.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting keras-tuner<1.0.2,>=1\n",
      "  Downloading keras-tuner-1.0.1.tar.gz (54 kB)\n",
      "\u001b[K     |████████████████████████████████| 54 kB 4.2 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: google-cloud-bigquery<3,>=1.28.0 in /opt/conda/lib/python3.7/site-packages (from tfx[kfp]==0.30.0) (2.20.0)\n",
      "Requirement already satisfied: grpcio<2,>=1.28.1 in /opt/conda/lib/python3.7/site-packages (from tfx[kfp]==0.30.0) (1.38.0)\n",
      "Requirement already satisfied: jinja2<3,>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from tfx[kfp]==0.30.0) (2.11.3)\n",
      "Collecting pyarrow<3,>=1\n",
      "  Downloading pyarrow-2.0.0-cp37-cp37m-manylinux2014_x86_64.whl (17.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 17.7 MB 43.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-model-analysis<0.31,>=0.30\n",
      "  Downloading tensorflow_model_analysis-0.30.0-py3-none-any.whl (1.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.7 MB 40.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-serving-api!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.5.*,<3,>=1.15\n",
      "  Downloading tensorflow_serving_api-2.4.1-py2.py3-none-any.whl (38 kB)\n",
      "Collecting ml-pipelines-sdk==0.30.0\n",
      "  Downloading ml_pipelines_sdk-0.30.0-py3-none-any.whl (1.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2 MB 57.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging<21,>=20 in /opt/conda/lib/python3.7/site-packages (from tfx[kfp]==0.30.0) (20.9)\n",
      "Collecting attrs<21,>=19.3.0\n",
      "  Downloading attrs-20.3.0-py2.py3-none-any.whl (49 kB)\n",
      "\u001b[K     |████████████████████████████████| 49 kB 7.9 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting docker<5,>=4.1\n",
      "  Downloading docker-4.4.4-py2.py3-none-any.whl (147 kB)\n",
      "\u001b[K     |████████████████████████████████| 147 kB 40.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting click<8,>=7\n",
      "  Downloading click-7.1.2-py2.py3-none-any.whl (82 kB)\n",
      "\u001b[K     |████████████████████████████████| 82 kB 1.6 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-hub<0.10,>=0.9.0\n",
      "  Downloading tensorflow_hub-0.9.0-py2.py3-none-any.whl (103 kB)\n",
      "\u001b[K     |████████████████████████████████| 103 kB 55.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyyaml<6,>=3.12 in /opt/conda/lib/python3.7/site-packages (from tfx[kfp]==0.30.0) (5.4.1)\n",
      "Collecting google-api-python-client<2,>=1.7.8\n",
      "  Downloading google_api_python_client-1.12.8-py2.py3-none-any.whl (61 kB)\n",
      "\u001b[K     |████████████████████████████████| 61 kB 43 kB/s s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-data-validation<0.31,>=0.30\n",
      "  Downloading tensorflow_data_validation-0.30.0-cp37-cp37m-manylinux2010_x86_64.whl (1.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.4 MB 30.4 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting tensorflow-transform<0.31,>=0.30\n",
      "  Downloading tensorflow_transform-0.30.0-py3-none-any.whl (398 kB)\n",
      "\u001b[K     |████████████████████████████████| 398 kB 47.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting absl-py<0.13,>=0.9\n",
      "  Downloading absl_py-0.12.0-py3-none-any.whl (129 kB)\n",
      "\u001b[K     |████████████████████████████████| 129 kB 55.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting apache-beam[gcp]<3,>=2.28\n",
      "  Downloading apache_beam-2.30.0-cp37-cp37m-manylinux2010_x86_64.whl (9.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 9.6 MB 18.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six<2,>=1.10 in /opt/conda/lib/python3.7/site-packages (from tfx[kfp]==0.30.0) (1.16.0)\n",
      "Collecting tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.5.*,<3,>=1.15.2\n",
      "  Downloading tensorflow-2.4.2-cp37-cp37m-manylinux2010_x86_64.whl (394.5 MB)\n",
      "\u001b[K     |████████████████████████████████| 394.5 MB 16 kB/s s eta 0:00:01��████████████████████▏       | 297.4 MB 58.1 MB/s eta 0:00:02\n",
      "\u001b[?25hCollecting portpicker<2,>=1.3.1\n",
      "  Downloading portpicker-1.4.0-py3-none-any.whl (13 kB)\n",
      "Collecting kfp-pipeline-spec<0.2,>=0.1.7\n",
      "  Downloading kfp_pipeline_spec-0.1.8-py3-none-any.whl (27 kB)\n",
      "Collecting kfp<2,>=1.1.0\n",
      "  Downloading kfp-1.6.4.tar.gz (225 kB)\n",
      "\u001b[K     |████████████████████████████████| 225 kB 51.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pydot<2,>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.28->tfx[kfp]==0.30.0) (1.4.2)\n",
      "Requirement already satisfied: pymongo<4.0.0,>=3.8.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.28->tfx[kfp]==0.30.0) (3.11.4)\n",
      "Collecting avro-python3!=1.9.2,<1.10.0,>=1.8.1\n",
      "  Downloading avro-python3-1.9.2.1.tar.gz (37 kB)\n",
      "Requirement already satisfied: hdfs<3.0.0,>=2.1.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.28->tfx[kfp]==0.30.0) (2.6.0)\n",
      "Requirement already satisfied: httplib2<0.20.0,>=0.8 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.28->tfx[kfp]==0.30.0) (0.19.1)\n",
      "Requirement already satisfied: pytz>=2018.3 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.28->tfx[kfp]==0.30.0) (2021.1)\n",
      "Requirement already satisfied: python-dateutil<3,>=2.8.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.28->tfx[kfp]==0.30.0) (2.8.1)\n",
      "Collecting typing-extensions<3.8.0,>=3.7.0\n",
      "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
      "Collecting dill<0.3.2,>=0.3.1.1\n",
      "  Downloading dill-0.3.1.1.tar.gz (151 kB)\n",
      "\u001b[K     |████████████████████████████████| 151 kB 40.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: crcmod<2.0,>=1.7 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.28->tfx[kfp]==0.30.0) (1.7)\n",
      "Requirement already satisfied: future<1.0.0,>=0.18.2 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.28->tfx[kfp]==0.30.0) (0.18.2)\n",
      "Requirement already satisfied: oauth2client<5,>=2.0.1 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.28->tfx[kfp]==0.30.0) (3.0.0)\n",
      "Requirement already satisfied: fastavro<2,>=0.21.4 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.28->tfx[kfp]==0.30.0) (0.21.24)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.24.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.28->tfx[kfp]==0.30.0) (2.25.1)\n",
      "Collecting google-cloud-vision<2,>=0.38.0\n",
      "  Downloading google_cloud_vision-1.0.0-py2.py3-none-any.whl (435 kB)\n",
      "\u001b[K     |████████████████████████████████| 435 kB 38.0 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-cloud-bigtable<2,>=0.31.1\n",
      "  Downloading google_cloud_bigtable-1.7.0-py2.py3-none-any.whl (267 kB)\n",
      "\u001b[K     |████████████████████████████████| 267 kB 49.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-apitools<0.5.32,>=0.5.31\n",
      "  Downloading google-apitools-0.5.31.tar.gz (173 kB)\n",
      "\u001b[K     |████████████████████████████████| 173 kB 48.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-cloud-videointelligence<2,>=1.8.0\n",
      "  Downloading google_cloud_videointelligence-1.16.1-py2.py3-none-any.whl (183 kB)\n",
      "\u001b[K     |████████████████████████████████| 183 kB 45.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: cachetools<5,>=3.1.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.28->tfx[kfp]==0.30.0) (4.2.2)\n",
      "Collecting google-cloud-spanner<2,>=1.13.0\n",
      "  Downloading google_cloud_spanner-1.19.1-py2.py3-none-any.whl (255 kB)\n",
      "\u001b[K     |████████████████████████████████| 255 kB 50.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-cloud-dlp<2,>=0.12.0\n",
      "  Downloading google_cloud_dlp-1.0.0-py2.py3-none-any.whl (169 kB)\n",
      "\u001b[K     |████████████████████████████████| 169 kB 46.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-cloud-datastore<2,>=1.7.1\n",
      "  Downloading google_cloud_datastore-1.15.3-py2.py3-none-any.whl (134 kB)\n",
      "\u001b[K     |████████████████████████████████| 134 kB 47.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: google-cloud-pubsub<2,>=0.39.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.28->tfx[kfp]==0.30.0) (1.7.0)\n",
      "Collecting google-cloud-language<2,>=1.3.0\n",
      "  Downloading google_cloud_language-1.3.0-py2.py3-none-any.whl (83 kB)\n",
      "\u001b[K     |████████████████████████████████| 83 kB 2.6 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: grpcio-gcp<1,>=0.2.2 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.28->tfx[kfp]==0.30.0) (0.2.2)\n",
      "Requirement already satisfied: google-cloud-core<2,>=0.28.1 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.28->tfx[kfp]==0.30.0) (1.7.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.18.0 in /opt/conda/lib/python3.7/site-packages (from apache-beam[gcp]<3,>=2.28->tfx[kfp]==0.30.0) (1.30.2)\n",
      "Requirement already satisfied: websocket-client>=0.32.0 in /opt/conda/lib/python3.7/site-packages (from docker<5,>=4.1->tfx[kfp]==0.30.0) (0.57.0)\n",
      "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client<2,>=1.7.8->tfx[kfp]==0.30.0) (0.1.0)\n",
      "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client<2,>=1.7.8->tfx[kfp]==0.30.0) (3.0.1)\n",
      "Requirement already satisfied: google-api-core<2dev,>=1.21.0 in /opt/conda/lib/python3.7/site-packages (from google-api-python-client<2,>=1.7.8->tfx[kfp]==0.30.0) (1.30.0)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client<2,>=1.7.8->tfx[kfp]==0.30.0) (49.6.0.post20210108)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from google-api-core<2dev,>=1.21.0->google-api-python-client<2,>=1.7.8->tfx[kfp]==0.30.0) (1.53.0)\n",
      "Requirement already satisfied: fasteners>=0.14 in /opt/conda/lib/python3.7/site-packages (from google-apitools<0.5.32,>=0.5.31->apache-beam[gcp]<3,>=2.28->tfx[kfp]==0.30.0) (0.16.3)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.18.0->apache-beam[gcp]<3,>=2.28->tfx[kfp]==0.30.0) (4.7.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<2,>=1.18.0->apache-beam[gcp]<3,>=2.28->tfx[kfp]==0.30.0) (0.2.7)\n",
      "Requirement already satisfied: proto-plus>=1.10.1 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform<0.8,>=0.5.0->tfx[kfp]==0.30.0) (1.18.1)\n",
      "Requirement already satisfied: google-cloud-storage<2.0.0dev,>=1.26.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform<0.8,>=0.5.0->tfx[kfp]==0.30.0) (1.38.0)\n",
      "Requirement already satisfied: google-resumable-media<2.0dev,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<3,>=1.28.0->tfx[kfp]==0.30.0) (1.3.1)\n",
      "Requirement already satisfied: grpc-google-iam-v1<0.13dev,>=0.12.3 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigtable<2,>=0.31.1->apache-beam[gcp]<3,>=2.28->tfx[kfp]==0.30.0) (0.12.3)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media<2.0dev,>=0.6.0->google-cloud-bigquery<3,>=1.28.0->tfx[kfp]==0.30.0) (1.1.2)\n",
      "Requirement already satisfied: cffi>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from google-crc32c<2.0dev,>=1.0->google-resumable-media<2.0dev,>=0.6.0->google-cloud-bigquery<3,>=1.28.0->tfx[kfp]==0.30.0) (1.14.5)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.7/site-packages (from cffi>=1.0.0->google-crc32c<2.0dev,>=1.0->google-resumable-media<2.0dev,>=0.6.0->google-cloud-bigquery<3,>=1.28.0->tfx[kfp]==0.30.0) (2.20)\n",
      "Requirement already satisfied: docopt in /opt/conda/lib/python3.7/site-packages (from hdfs<3.0.0,>=2.1.0->apache-beam[gcp]<3,>=2.28->tfx[kfp]==0.30.0) (0.6.2)\n",
      "Requirement already satisfied: pyparsing<3,>=2.4.2 in /opt/conda/lib/python3.7/site-packages (from httplib2<0.20.0,>=0.8->apache-beam[gcp]<3,>=2.28->tfx[kfp]==0.30.0) (2.4.7)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /opt/conda/lib/python3.7/site-packages (from jinja2<3,>=2.7.3->tfx[kfp]==0.30.0) (1.1.1)\n",
      "Collecting tabulate\n",
      "  Downloading tabulate-0.8.9-py3-none-any.whl (25 kB)\n",
      "Collecting terminaltables\n",
      "  Downloading terminaltables-3.1.0.tar.gz (12 kB)\n",
      "Requirement already satisfied: colorama in /opt/conda/lib/python3.7/site-packages (from keras-tuner<1.0.2,>=1->tfx[kfp]==0.30.0) (0.4.4)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from keras-tuner<1.0.2,>=1->tfx[kfp]==0.30.0) (4.61.1)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from keras-tuner<1.0.2,>=1->tfx[kfp]==0.30.0) (1.6.3)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.7/site-packages (from keras-tuner<1.0.2,>=1->tfx[kfp]==0.30.0) (0.24.2)\n",
      "Collecting absl-py<0.13,>=0.9\n",
      "  Downloading absl_py-0.11.0-py3-none-any.whl (127 kB)\n",
      "\u001b[K     |████████████████████████████████| 127 kB 48.2 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting requests-toolbelt<1,>=0.8.0\n",
      "  Downloading requests_toolbelt-0.9.1-py2.py3-none-any.whl (54 kB)\n",
      "\u001b[K     |████████████████████████████████| 54 kB 3.8 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: cloudpickle<2,>=1.3.0 in /opt/conda/lib/python3.7/site-packages (from kfp<2,>=1.1.0->tfx[kfp]==0.30.0) (1.6.0)\n",
      "Collecting kfp-server-api<2.0.0,>=1.1.2\n",
      "  Downloading kfp-server-api-1.6.0.tar.gz (52 kB)\n",
      "\u001b[K     |████████████████████████████████| 52 kB 1.9 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: jsonschema<4,>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from kfp<2,>=1.1.0->tfx[kfp]==0.30.0) (3.2.0)\n",
      "Collecting Deprecated<2,>=1.2.7\n",
      "  Downloading Deprecated-1.2.12-py2.py3-none-any.whl (9.5 kB)\n",
      "Collecting strip-hints<1,>=0.1.8\n",
      "  Downloading strip-hints-0.1.9.tar.gz (30 kB)\n",
      "Collecting docstring-parser<1,>=0.7.3\n",
      "  Downloading docstring_parser-0.8.1.tar.gz (14 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting fire<1,>=0.3.1\n",
      "  Downloading fire-0.4.0.tar.gz (87 kB)\n",
      "\u001b[K     |████████████████████████████████| 87 kB 7.6 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: wrapt<2,>=1.10 in /opt/conda/lib/python3.7/site-packages (from Deprecated<2,>=1.2.7->kfp<2,>=1.1.0->tfx[kfp]==0.30.0) (1.12.1)\n",
      "Requirement already satisfied: termcolor in /opt/conda/lib/python3.7/site-packages (from fire<1,>=0.3.1->kfp<2,>=1.1.0->tfx[kfp]==0.30.0) (1.1.0)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from jsonschema<4,>=3.0.1->kfp<2,>=1.1.0->tfx[kfp]==0.30.0) (4.5.0)\n",
      "Requirement already satisfied: pyrsistent>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema<4,>=3.0.1->kfp<2,>=1.1.0->tfx[kfp]==0.30.0) (0.17.3)\n",
      "Requirement already satisfied: urllib3>=1.15 in /opt/conda/lib/python3.7/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp<2,>=1.1.0->tfx[kfp]==0.30.0) (1.26.5)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.7/site-packages (from kfp-server-api<2.0.0,>=1.1.2->kfp<2,>=1.1.0->tfx[kfp]==0.30.0) (2021.5.30)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.7/site-packages (from kubernetes<12,>=10.0.1->tfx[kfp]==0.30.0) (1.3.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in /opt/conda/lib/python3.7/site-packages (from oauth2client<5,>=2.0.1->apache-beam[gcp]<3,>=2.28->tfx[kfp]==0.30.0) (0.4.8)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]<3,>=2.28->tfx[kfp]==0.30.0) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0,>=2.24.0->apache-beam[gcp]<3,>=2.28->tfx[kfp]==0.30.0) (2.10)\n",
      "Requirement already satisfied: wheel in /opt/conda/lib/python3.7/site-packages (from strip-hints<1,>=0.1.8->kfp<2,>=1.1.0->tfx[kfp]==0.30.0) (0.36.2)\n",
      "Collecting six<2,>=1.10\n",
      "  Downloading six-1.15.0-py2.py3-none-any.whl (10 kB)\n",
      "Collecting keras-preprocessing~=1.1.2\n",
      "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 1.7 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: google-pasta~=0.2 in /opt/conda/lib/python3.7/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.5.*,<3,>=1.15.2->tfx[kfp]==0.30.0) (0.2.0)\n",
      "Collecting tensorflow-estimator<2.5.0,>=2.4.0\n",
      "  Downloading tensorflow_estimator-2.4.0-py2.py3-none-any.whl (462 kB)\n",
      "\u001b[K     |████████████████████████████████| 462 kB 46.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting astunparse~=1.6.3\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: h5py~=2.10.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.5.*,<3,>=1.15.2->tfx[kfp]==0.30.0) (2.10.0)\n",
      "Collecting flatbuffers~=1.12.0\n",
      "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
      "Collecting gast==0.3.3\n",
      "  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n",
      "Requirement already satisfied: opt-einsum~=3.3.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.5.*,<3,>=1.15.2->tfx[kfp]==0.30.0) (3.3.0)\n",
      "Collecting grpcio<2,>=1.28.1\n",
      "  Downloading grpcio-1.32.0-cp37-cp37m-manylinux2014_x86_64.whl (3.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.8 MB 45.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tensorboard~=2.4 in /opt/conda/lib/python3.7/site-packages (from tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.5.*,<3,>=1.15.2->tfx[kfp]==0.30.0) (2.5.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.5.*,<3,>=1.15.2->tfx[kfp]==0.30.0) (3.3.4)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.5.*,<3,>=1.15.2->tfx[kfp]==0.30.0) (1.8.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.5.*,<3,>=1.15.2->tfx[kfp]==0.30.0) (2.0.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.5.*,<3,>=1.15.2->tfx[kfp]==0.30.0) (0.6.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.5.*,<3,>=1.15.2->tfx[kfp]==0.30.0) (0.4.4)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib->kubernetes<12,>=10.0.1->tfx[kfp]==0.30.0) (3.1.1)\n",
      "Requirement already satisfied: pandas<2,>=1.0 in /opt/conda/lib/python3.7/site-packages (from tensorflow-data-validation<0.31,>=0.30->tfx[kfp]==0.30.0) (1.2.4)\n",
      "Collecting tensorflow-metadata<0.31,>=0.30\n",
      "  Downloading tensorflow_metadata-0.30.0-py3-none-any.whl (47 kB)\n",
      "\u001b[K     |████████████████████████████████| 47 kB 7.0 MB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting joblib<0.15,>=0.12\n",
      "  Downloading joblib-0.14.1-py2.py3-none-any.whl (294 kB)\n",
      "\u001b[K     |████████████████████████████████| 294 kB 50.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: ipywidgets<8,>=7 in /opt/conda/lib/python3.7/site-packages (from tensorflow-model-analysis<0.31,>=0.30->tfx[kfp]==0.30.0) (7.6.3)\n",
      "Requirement already satisfied: ipython<8,>=7 in /opt/conda/lib/python3.7/site-packages (from tensorflow-model-analysis<0.31,>=0.30->tfx[kfp]==0.30.0) (7.24.1)\n",
      "Requirement already satisfied: traitlets>=4.2 in /opt/conda/lib/python3.7/site-packages (from ipython<8,>=7->tensorflow-model-analysis<0.31,>=0.30->tfx[kfp]==0.30.0) (5.0.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from ipython<8,>=7->tensorflow-model-analysis<0.31,>=0.30->tfx[kfp]==0.30.0) (3.0.19)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.7/site-packages (from ipython<8,>=7->tensorflow-model-analysis<0.31,>=0.30->tfx[kfp]==0.30.0) (5.0.9)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.7/site-packages (from ipython<8,>=7->tensorflow-model-analysis<0.31,>=0.30->tfx[kfp]==0.30.0) (0.1.2)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.7/site-packages (from ipython<8,>=7->tensorflow-model-analysis<0.31,>=0.30->tfx[kfp]==0.30.0) (0.18.0)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.7/site-packages (from ipython<8,>=7->tensorflow-model-analysis<0.31,>=0.30->tfx[kfp]==0.30.0) (0.7.5)\n",
      "Requirement already satisfied: pygments in /opt/conda/lib/python3.7/site-packages (from ipython<8,>=7->tensorflow-model-analysis<0.31,>=0.30->tfx[kfp]==0.30.0) (2.9.0)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.7/site-packages (from ipython<8,>=7->tensorflow-model-analysis<0.31,>=0.30->tfx[kfp]==0.30.0) (0.2.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.7/site-packages (from ipython<8,>=7->tensorflow-model-analysis<0.31,>=0.30->tfx[kfp]==0.30.0) (4.8.0)\n",
      "Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.31,>=0.30->tfx[kfp]==0.30.0) (1.0.0)\n",
      "Requirement already satisfied: widgetsnbextension~=3.5.0 in /opt/conda/lib/python3.7/site-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.31,>=0.30->tfx[kfp]==0.30.0) (3.5.1)\n",
      "Requirement already satisfied: ipykernel>=4.5.1 in /opt/conda/lib/python3.7/site-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.31,>=0.30->tfx[kfp]==0.30.0) (5.5.5)\n",
      "Requirement already satisfied: nbformat>=4.2.0 in /opt/conda/lib/python3.7/site-packages (from ipywidgets<8,>=7->tensorflow-model-analysis<0.31,>=0.30->tfx[kfp]==0.30.0) (5.1.3)\n",
      "Requirement already satisfied: tornado>=4.2 in /opt/conda/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets<8,>=7->tensorflow-model-analysis<0.31,>=0.30->tfx[kfp]==0.30.0) (6.1)\n",
      "Requirement already satisfied: jupyter-client in /opt/conda/lib/python3.7/site-packages (from ipykernel>=4.5.1->ipywidgets<8,>=7->tensorflow-model-analysis<0.31,>=0.30->tfx[kfp]==0.30.0) (6.1.12)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.7/site-packages (from jedi>=0.16->ipython<8,>=7->tensorflow-model-analysis<0.31,>=0.30->tfx[kfp]==0.30.0) (0.8.2)\n",
      "Requirement already satisfied: jupyter-core in /opt/conda/lib/python3.7/site-packages (from nbformat>=4.2.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.31,>=0.30->tfx[kfp]==0.30.0) (4.7.1)\n",
      "Requirement already satisfied: ipython-genutils in /opt/conda/lib/python3.7/site-packages (from nbformat>=4.2.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.31,>=0.30->tfx[kfp]==0.30.0) (0.2.0)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.7/site-packages (from pexpect>4.3->ipython<8,>=7->tensorflow-model-analysis<0.31,>=0.30->tfx[kfp]==0.30.0) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.7/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython<8,>=7->tensorflow-model-analysis<0.31,>=0.30->tfx[kfp]==0.30.0) (0.2.5)\n",
      "Requirement already satisfied: notebook>=4.4.1 in /opt/conda/lib/python3.7/site-packages (from widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.31,>=0.30->tfx[kfp]==0.30.0) (6.4.0)\n",
      "Requirement already satisfied: argon2-cffi in /opt/conda/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.31,>=0.30->tfx[kfp]==0.30.0) (20.1.0)\n",
      "Requirement already satisfied: nbconvert in /opt/conda/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.31,>=0.30->tfx[kfp]==0.30.0) (6.0.7)\n",
      "Requirement already satisfied: Send2Trash>=1.5.0 in /opt/conda/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.31,>=0.30->tfx[kfp]==0.30.0) (1.5.0)\n",
      "Requirement already satisfied: pyzmq>=17 in /opt/conda/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.31,>=0.30->tfx[kfp]==0.30.0) (22.1.0)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /opt/conda/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.31,>=0.30->tfx[kfp]==0.30.0) (0.10.1)\n",
      "Requirement already satisfied: prometheus-client in /opt/conda/lib/python3.7/site-packages (from notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.31,>=0.30->tfx[kfp]==0.30.0) (0.11.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->jsonschema<4,>=3.0.1->kfp<2,>=1.1.0->tfx[kfp]==0.30.0) (3.4.1)\n",
      "Requirement already satisfied: mistune<2,>=0.8.1 in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.31,>=0.30->tfx[kfp]==0.30.0) (0.8.4)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.31,>=0.30->tfx[kfp]==0.30.0) (0.3)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.31,>=0.30->tfx[kfp]==0.30.0) (1.4.2)\n",
      "Requirement already satisfied: bleach in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.31,>=0.30->tfx[kfp]==0.30.0) (3.3.0)\n",
      "Requirement already satisfied: defusedxml in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.31,>=0.30->tfx[kfp]==0.30.0) (0.7.1)\n",
      "Requirement already satisfied: testpath in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.31,>=0.30->tfx[kfp]==0.30.0) (0.5.0)\n",
      "Requirement already satisfied: jupyterlab-pygments in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.31,>=0.30->tfx[kfp]==0.30.0) (0.1.2)\n",
      "Requirement already satisfied: nbclient<0.6.0,>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.31,>=0.30->tfx[kfp]==0.30.0) (0.5.3)\n",
      "Requirement already satisfied: nest-asyncio in /opt/conda/lib/python3.7/site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.31,>=0.30->tfx[kfp]==0.30.0) (1.5.1)\n",
      "Requirement already satisfied: async-generator in /opt/conda/lib/python3.7/site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.31,>=0.30->tfx[kfp]==0.30.0) (1.10)\n",
      "Requirement already satisfied: webencodings in /opt/conda/lib/python3.7/site-packages (from bleach->nbconvert->notebook>=4.4.1->widgetsnbextension~=3.5.0->ipywidgets<8,>=7->tensorflow-model-analysis<0.31,>=0.30->tfx[kfp]==0.30.0) (0.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit-learn->keras-tuner<1.0.2,>=1->tfx[kfp]==0.30.0) (2.1.0)\n",
      "Building wheels for collected packages: avro-python3, dill, google-apitools, keras-tuner, kfp, docstring-parser, fire, kfp-server-api, strip-hints, terminaltables\n",
      "  Building wheel for avro-python3 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for avro-python3: filename=avro_python3-1.9.2.1-py3-none-any.whl size=43512 sha256=69aef32ec13468f918cd044d04e3401625acf5434c9617b4a557e685b5c4f3cf\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/bc/49/5f/fdb5b9d85055c478213e0158ac122b596816149a02d82e0ab1\n",
      "  Building wheel for dill (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for dill: filename=dill-0.3.1.1-py3-none-any.whl size=78532 sha256=024f26ef9ff6d9d1f94238c3e2a4d64e66535e90c556f541b18c8e4dd86e5281\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/a4/61/fd/c57e374e580aa78a45ed78d5859b3a44436af17e22ca53284f\n",
      "  Building wheel for google-apitools (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for google-apitools: filename=google_apitools-0.5.31-py3-none-any.whl size=131041 sha256=b64215d5c6cba8f5494498537350b73d9b4e81182147247ccf161cc7ba0be40c\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/19/b5/2f/1cc3cf2b31e7a9cd1508731212526d9550271274d351c96f16\n",
      "  Building wheel for keras-tuner (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for keras-tuner: filename=keras_tuner-1.0.1-py3-none-any.whl size=73199 sha256=1aef9f938ad880b43a69777bd39326685fcaee4ec5504f04461228235c09d768\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/0b/cf/2f/1a1749d3a3650fac3305a8d7f9237b6de7c41068e2f8520ca2\n",
      "  Building wheel for kfp (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kfp: filename=kfp-1.6.4-py3-none-any.whl size=307978 sha256=4b5abc82166e5323adfbcc8e864451417937f772cc465fc81e087a11f0d802d3\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/68/fb/ec/d4c6e6c3a85acbf75cf4a868faf8f2fe98b75973c223f8f10c\n",
      "  Building wheel for docstring-parser (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for docstring-parser: filename=docstring_parser-0.8.1-py3-none-any.whl size=19678 sha256=3e953f4323698e2456bc17701b04fb4ed2c971e7796dba3b104e219d09d9b2eb\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/64/11/a5/143eba02bbc268191378c202be131d94b59ca40270bd49dd43\n",
      "  Building wheel for fire (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fire: filename=fire-0.4.0-py2.py3-none-any.whl size=115928 sha256=4f177ce1c4e7a12e221bca73547951ba7fbf3297383410ec154f2f43b2db8ae3\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/8a/67/fb/2e8a12fa16661b9d5af1f654bd199366799740a85c64981226\n",
      "  Building wheel for kfp-server-api (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kfp-server-api: filename=kfp_server_api-1.6.0-py3-none-any.whl size=92524 sha256=56179b32b27262ee29696822b0c690c1336c5cbd007beadc2909a3ef77bd75ec\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/25/2f/7c/d5c1cbcb535e30c90b88aa5104b1ee14a0ea9313a543bfdf52\n",
      "  Building wheel for strip-hints (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for strip-hints: filename=strip_hints-0.1.9-py2.py3-none-any.whl size=20993 sha256=c9b3fd722cc21dd41838a9843bdae5a2e5968a51ec07b93972506b9e0d39baa7\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/2d/b8/4e/a3ec111d2db63cec88121bd7c0ab1a123bce3b55dd19dda5c1\n",
      "  Building wheel for terminaltables (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for terminaltables: filename=terminaltables-3.1.0-py3-none-any.whl size=15355 sha256=721785b76a17755fa31326ab15a7ccfbbb83eac99a1bf095c217736fd1eee0ba\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/ba/ad/c8/2d98360791161cd3db6daf6b5e730f34021fc9367d5879f497\n",
      "Successfully built avro-python3 dill google-apitools keras-tuner kfp docstring-parser fire kfp-server-api strip-hints terminaltables\n",
      "Installing collected packages: typing-extensions, six, attrs, grpcio, absl-py, tensorflow-estimator, pyarrow, keras-preprocessing, gast, flatbuffers, dill, avro-python3, astunparse, tensorflow, google-cloud-vision, google-cloud-videointelligence, google-cloud-spanner, google-cloud-language, google-cloud-dlp, google-cloud-datastore, google-cloud-bigtable, google-apitools, apache-beam, tensorflow-serving-api, tensorflow-metadata, joblib, google-api-python-client, tfx-bsl, terminaltables, tabulate, ml-metadata, docker, tensorflow-transform, tensorflow-model-analysis, tensorflow-hub, tensorflow-data-validation, strip-hints, requests-toolbelt, portpicker, ml-pipelines-sdk, kubernetes, kfp-server-api, kfp-pipeline-spec, keras-tuner, google-cloud-aiplatform, fire, docstring-parser, Deprecated, click, tfx, kfp\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing-extensions 3.10.0.0\n",
      "    Uninstalling typing-extensions-3.10.0.0:\n",
      "      Successfully uninstalled typing-extensions-3.10.0.0\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.16.0\n",
      "    Uninstalling six-1.16.0:\n",
      "      Successfully uninstalled six-1.16.0\n",
      "  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 21.2.0\n",
      "    Uninstalling attrs-21.2.0:\n",
      "      Successfully uninstalled attrs-21.2.0\n",
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.38.0\n",
      "    Uninstalling grpcio-1.38.0:\n",
      "      Successfully uninstalled grpcio-1.38.0\n",
      "  Attempting uninstall: absl-py\n",
      "    Found existing installation: absl-py 0.8.1\n",
      "    Uninstalling absl-py-0.8.1:\n",
      "      Successfully uninstalled absl-py-0.8.1\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "    Found existing installation: tensorflow-estimator 2.1.0\n",
      "    Uninstalling tensorflow-estimator-2.1.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.1.0\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 4.0.1\n",
      "    Uninstalling pyarrow-4.0.1:\n",
      "      Successfully uninstalled pyarrow-4.0.1\n",
      "  Attempting uninstall: keras-preprocessing\n",
      "    Found existing installation: Keras-Preprocessing 1.1.0\n",
      "    Uninstalling Keras-Preprocessing-1.1.0:\n",
      "      Successfully uninstalled Keras-Preprocessing-1.1.0\n",
      "  Attempting uninstall: gast\n",
      "    Found existing installation: gast 0.2.2\n",
      "    Uninstalling gast-0.2.2:\n",
      "      Successfully uninstalled gast-0.2.2\n",
      "  Attempting uninstall: dill\n",
      "    Found existing installation: dill 0.3.0\n",
      "    Uninstalling dill-0.3.0:\n",
      "      Successfully uninstalled dill-0.3.0\n",
      "  Attempting uninstall: avro-python3\n",
      "    Found existing installation: avro-python3 1.10.2\n",
      "    Uninstalling avro-python3-1.10.2:\n",
      "      Successfully uninstalled avro-python3-1.10.2\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.1.4\n",
      "    Uninstalling tensorflow-2.1.4:\n",
      "      Successfully uninstalled tensorflow-2.1.4\n",
      "  Attempting uninstall: google-cloud-vision\n",
      "    Found existing installation: google-cloud-vision 2.3.2\n",
      "    Uninstalling google-cloud-vision-2.3.2:\n",
      "      Successfully uninstalled google-cloud-vision-2.3.2\n",
      "  Attempting uninstall: google-cloud-videointelligence\n",
      "    Found existing installation: google-cloud-videointelligence 2.2.0\n",
      "    Uninstalling google-cloud-videointelligence-2.2.0:\n",
      "      Successfully uninstalled google-cloud-videointelligence-2.2.0\n",
      "  Attempting uninstall: google-cloud-spanner\n",
      "    Found existing installation: google-cloud-spanner 3.5.0\n",
      "    Uninstalling google-cloud-spanner-3.5.0:\n",
      "      Successfully uninstalled google-cloud-spanner-3.5.0\n",
      "  Attempting uninstall: google-cloud-language\n",
      "    Found existing installation: google-cloud-language 2.1.0\n",
      "    Uninstalling google-cloud-language-2.1.0:\n",
      "      Successfully uninstalled google-cloud-language-2.1.0\n",
      "  Attempting uninstall: google-cloud-datastore\n",
      "    Found existing installation: google-cloud-datastore 2.1.3\n",
      "    Uninstalling google-cloud-datastore-2.1.3:\n",
      "      Successfully uninstalled google-cloud-datastore-2.1.3\n",
      "  Attempting uninstall: google-cloud-bigtable\n",
      "    Found existing installation: google-cloud-bigtable 2.2.0\n",
      "    Uninstalling google-cloud-bigtable-2.2.0:\n",
      "      Successfully uninstalled google-cloud-bigtable-2.2.0\n",
      "  Attempting uninstall: google-apitools\n",
      "    Found existing installation: google-apitools 0.5.28\n",
      "    Uninstalling google-apitools-0.5.28:\n",
      "      Successfully uninstalled google-apitools-0.5.28\n",
      "  Attempting uninstall: apache-beam\n",
      "    Found existing installation: apache-beam 2.17.0\n",
      "    Uninstalling apache-beam-2.17.0:\n",
      "      Successfully uninstalled apache-beam-2.17.0\n",
      "  Attempting uninstall: tensorflow-serving-api\n",
      "    Found existing installation: tensorflow-serving-api 2.1.0\n",
      "    Uninstalling tensorflow-serving-api-2.1.0:\n",
      "      Successfully uninstalled tensorflow-serving-api-2.1.0\n",
      "  Attempting uninstall: tensorflow-metadata\n",
      "    Found existing installation: tensorflow-metadata 0.21.2\n",
      "    Uninstalling tensorflow-metadata-0.21.2:\n",
      "      Successfully uninstalled tensorflow-metadata-0.21.2\n",
      "  Attempting uninstall: joblib\n",
      "    Found existing installation: joblib 1.0.1\n",
      "    Uninstalling joblib-1.0.1:\n",
      "      Successfully uninstalled joblib-1.0.1\n",
      "  Attempting uninstall: google-api-python-client\n",
      "    Found existing installation: google-api-python-client 2.9.0\n",
      "    Uninstalling google-api-python-client-2.9.0:\n",
      "      Successfully uninstalled google-api-python-client-2.9.0\n",
      "  Attempting uninstall: tfx-bsl\n",
      "    Found existing installation: tfx-bsl 0.21.4\n",
      "    Uninstalling tfx-bsl-0.21.4:\n",
      "      Successfully uninstalled tfx-bsl-0.21.4\n",
      "  Attempting uninstall: ml-metadata\n",
      "    Found existing installation: ml-metadata 0.21.2\n",
      "    Uninstalling ml-metadata-0.21.2:\n",
      "      Successfully uninstalled ml-metadata-0.21.2\n",
      "  Attempting uninstall: docker\n",
      "    Found existing installation: docker 5.0.0\n",
      "    Uninstalling docker-5.0.0:\n",
      "      Successfully uninstalled docker-5.0.0\n",
      "  Attempting uninstall: tensorflow-transform\n",
      "    Found existing installation: tensorflow-transform 0.21.2\n",
      "    Uninstalling tensorflow-transform-0.21.2:\n",
      "      Successfully uninstalled tensorflow-transform-0.21.2\n",
      "  Attempting uninstall: tensorflow-model-analysis\n",
      "    Found existing installation: tensorflow-model-analysis 0.21.6\n",
      "    Uninstalling tensorflow-model-analysis-0.21.6:\n",
      "      Successfully uninstalled tensorflow-model-analysis-0.21.6\n",
      "  Attempting uninstall: tensorflow-hub\n",
      "    Found existing installation: tensorflow-hub 0.7.0\n",
      "    Uninstalling tensorflow-hub-0.7.0:\n",
      "      Successfully uninstalled tensorflow-hub-0.7.0\n",
      "  Attempting uninstall: tensorflow-data-validation\n",
      "    Found existing installation: tensorflow-data-validation 0.21.5\n",
      "    Uninstalling tensorflow-data-validation-0.21.5:\n",
      "      Successfully uninstalled tensorflow-data-validation-0.21.5\n",
      "  Attempting uninstall: kubernetes\n",
      "    Found existing installation: kubernetes 17.17.0\n",
      "    Uninstalling kubernetes-17.17.0:\n",
      "      Successfully uninstalled kubernetes-17.17.0\n",
      "  Attempting uninstall: keras-tuner\n",
      "    Found existing installation: keras-tuner 1.0.3\n",
      "    Uninstalling keras-tuner-1.0.3:\n",
      "      Successfully uninstalled keras-tuner-1.0.3\n",
      "  Attempting uninstall: click\n",
      "    Found existing installation: click 8.0.1\n",
      "    Uninstalling click-8.0.1:\n",
      "      Successfully uninstalled click-8.0.1\n",
      "  Attempting uninstall: tfx\n",
      "    Found existing installation: tfx 0.21.5\n",
      "    Uninstalling tfx-0.21.5:\n",
      "      Successfully uninstalled tfx-0.21.5\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "witwidget 1.7.0 requires oauth2client>=4.1.3, but you have oauth2client 3.0.0 which is incompatible.\n",
      "tensorflow-io 0.11.0 requires tensorflow==2.1.0, but you have tensorflow 2.4.2 which is incompatible.\u001b[0m\n",
      "Successfully installed Deprecated-1.2.12 absl-py-0.11.0 apache-beam-2.30.0 astunparse-1.6.3 attrs-20.3.0 avro-python3-1.9.2.1 click-7.1.2 dill-0.3.1.1 docker-4.4.4 docstring-parser-0.8.1 fire-0.4.0 flatbuffers-1.12 gast-0.3.3 google-api-python-client-1.12.8 google-apitools-0.5.31 google-cloud-aiplatform-0.7.1 google-cloud-bigtable-1.7.0 google-cloud-datastore-1.15.3 google-cloud-dlp-1.0.0 google-cloud-language-1.3.0 google-cloud-spanner-1.19.1 google-cloud-videointelligence-1.16.1 google-cloud-vision-1.0.0 grpcio-1.32.0 joblib-0.14.1 keras-preprocessing-1.1.2 keras-tuner-1.0.1 kfp-1.6.4 kfp-pipeline-spec-0.1.8 kfp-server-api-1.6.0 kubernetes-11.0.0 ml-metadata-0.30.0 ml-pipelines-sdk-0.30.0 portpicker-1.4.0 pyarrow-2.0.0 requests-toolbelt-0.9.1 six-1.15.0 strip-hints-0.1.9 tabulate-0.8.9 tensorflow-2.4.2 tensorflow-data-validation-0.30.0 tensorflow-estimator-2.4.0 tensorflow-hub-0.9.0 tensorflow-metadata-0.30.0 tensorflow-model-analysis-0.30.0 tensorflow-serving-api-2.4.1 tensorflow-transform-0.30.0 terminaltables-3.1.0 tfx-0.30.0 tfx-bsl-0.30.0 typing-extensions-3.7.4.3\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "# Use the latest version of pip.\n",
    "!pip install --upgrade pip\n",
    "# Install tfx and kfp Python packages.\n",
    "!pip install --upgrade tfx[kfp]==0.30.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Hw3nsooU0okv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: GOOGLE_CLOUD_PROJECT=astute-pride-317802\n",
      "GCP project ID:astute-pride-317802\n"
     ]
    }
   ],
   "source": [
    "# Read GCP project id from env.\n",
    "shell_output=!gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "GOOGLE_CLOUD_PROJECT=shell_output[0]\n",
    "%env GOOGLE_CLOUD_PROJECT={GOOGLE_CLOUD_PROJECT}\n",
    "print(\"GCP project ID:\" + GOOGLE_CLOUD_PROJECT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "AzqEQORV0oky"
   },
   "outputs": [],
   "source": [
    "# This refers to the KFP cluster endpoint\n",
    "ENDPOINT='314cd0b9ca67984e-dot-asia-east1.pipelines.googleusercontent.com' # Enter your ENDPOINT here.\n",
    "if not ENDPOINT:\n",
    "    from absl import logging\n",
    "    logging.error('Set your ENDPOINT in this cell.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "3ztxXOVD0ok4"
   },
   "outputs": [],
   "source": [
    "# Docker image name for the pipeline image.\n",
    "CUSTOM_TFX_IMAGE='gcr.io/' + GOOGLE_CLOUD_PROJECT + '/tfx-pipeline'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "cIPlt-700ok-"
   },
   "outputs": [],
   "source": [
    "PIPELINE_NAME=\"data_centric_pipeline\"\n",
    "import os\n",
    "PROJECT_DIR=os.path.join(os.path.expanduser(\"~\"),\"data-centric\",PIPELINE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ROwHAsDK0olT"
   },
   "source": [
    "## Testing models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "M0cMdE2Z0olU",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-25 23:40:56.219453: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "Running tests under Python 3.7.10: /opt/conda/bin/python\n",
      "[ RUN      ] PreprocessingTest.testPreprocessingFn\n",
      "INFO:tensorflow:time(__main__.PreprocessingTest.testPreprocessingFn): 0.0s\n",
      "I0625 23:40:58.301542 140712543123264 test_util.py:2076] time(__main__.PreprocessingTest.testPreprocessingFn): 0.0s\n",
      "[       OK ] PreprocessingTest.testPreprocessingFn\n",
      "[ RUN      ] PreprocessingTest.test_session\n",
      "[  SKIPPED ] PreprocessingTest.test_session\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 0.001s\n",
      "\n",
      "OK (skipped=1)\n",
      "/opt/conda/bin/python: No module named models.keras.model_test\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m models.preprocessing_test\n",
    "!{sys.executable} -m models.keras.model_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tO9Jhplo0olX"
   },
   "source": [
    "## Create first TFX pipeline\n",
    "copy data to storage\n",
    "execute after convert data into tfrecords (interactive notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "gW-dSHW-TSdc",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://data/train/i.tfrecords [Content-Type=application/octet-stream]...\n",
      "Copying file://data/train/iv.tfrecords [Content-Type=application/octet-stream]...\n",
      "Copying file://data/train/x.tfrecords [Content-Type=application/octet-stream]...\n",
      "Copying file://data/train/vi.tfrecords [Content-Type=application/octet-stream]...\n",
      "Copying file://data/train/ix.tfrecords [Content-Type=application/octet-stream]...\n",
      "Copying file://data/train/v.tfrecords [Content-Type=application/octet-stream]...\n",
      "Copying file://data/train/iii.tfrecords [Content-Type=application/octet-stream]...\n",
      "Copying file://data/train/viii.tfrecords [Content-Type=application/octet-stream]...\n",
      "Copying file://data/train/ii.tfrecords [Content-Type=application/octet-stream]...\n",
      "Copying file://data/train/vii.tfrecords [Content-Type=application/octet-stream]...\n",
      "/ [10/10 files][814.2 KiB/814.2 KiB] 100% Done                                  \n",
      "Operation completed over 10 objects/814.2 KiB.                                   \n"
     ]
    }
   ],
   "source": [
    "!gsutil -m cp -r data/train/ gs://{GOOGLE_CLOUD_PROJECT}-kubeflowpipelines-default/data-centric/data/train/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://data/val/x.tfrecords [Content-Type=application/octet-stream]...\n",
      "Copying file://data/val/vi.tfrecords [Content-Type=application/octet-stream]... \n",
      "Copying file://data/val/i.tfrecords [Content-Type=application/octet-stream]...  \n",
      "Copying file://data/val/ix.tfrecords [Content-Type=application/octet-stream]...\n",
      "Copying file://data/val/iii.tfrecords [Content-Type=application/octet-stream]...\n",
      "Copying file://data/val/iv.tfrecords [Content-Type=application/octet-stream]... \n",
      "Copying file://data/val/ii.tfrecords [Content-Type=application/octet-stream]... \n",
      "Copying file://data/val/v.tfrecords [Content-Type=application/octet-stream]...  \n",
      "Copying file://data/val/viii.tfrecords [Content-Type=application/octet-stream]...\n",
      "Copying file://data/val/vii.tfrecords [Content-Type=application/octet-stream]...\n",
      "/ [10/10 files][250.2 KiB/250.2 KiB] 100% Done                                  \n",
      "Operation completed over 10 objects/250.2 KiB.                                   \n"
     ]
    }
   ],
   "source": [
    "!gsutil -m cp -r data/val/ gs://{GOOGLE_CLOUD_PROJECT}-kubeflowpipelines-default/data-centric/data/val/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wc54hDZu0ole"
   },
   "source": [
    "## build docker image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "kOU7zQof0olf",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-26 03:50:05.301928: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "WARNING:absl:RuntimeParameter is only supported on Cloud-based DAG runner currently.\n",
      "CLI\n",
      "Creating pipeline\n",
      "Detected Kubeflow.\n",
      "Use --engine flag if you intend to use a different orchestrator.\n",
      "/opt/conda/lib/python3.7/site-packages/kfp/_client.py:182: UserWarning: The host 314cd0b9ca67984e-dot-asia-east1.pipelines.googleusercontent.com does not contain the \"http\" or \"https\" protocol. Defaults to \"https\".\n",
      "  ' Defaults to \"https\".' % host)\n",
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "[Docker] Step 1/4 : FROM tensorflow/tfx:0.30.0[Docker] \n",
      "[Docker]  ---> 68adb9229d27\n",
      "[Docker] Step 2/4 : WORKDIR /pipeline[Docker] \n",
      "[Docker]  ---> Using cache\n",
      "[Docker]  ---> 62adad88abd0\n",
      "[Docker] Step 3/4 : COPY ./ ./[Docker] \n",
      "[Docker]  ---> 5a2d0f74d12b\n",
      "[Docker] Step 4/4 : ENV PYTHONPATH=\"/pipeline:${PYTHONPATH}\"[Docker] \n",
      "[Docker]  ---> Running in cbea82a96a54\n",
      "[Docker] Removing intermediate container cbea82a96a54\n",
      "[Docker]  ---> d0a66ec771bc\n",
      "[Docker] Successfully built d0a66ec771bc\n",
      "[Docker] Successfully tagged gcr.io/astute-pride-317802/data_centric_pipeline:latest\n",
      "[Docker] The push refers to repository [gcr.io/astute-pride-317802/data_centric_pipeline]\n",
      "[Docker] Preparing\n",
      "[Docker] Preparing\n",
      "[Docker] Preparing\n",
      "[Docker] Preparing\n",
      "[Docker] Preparing\n",
      "[Docker] Preparing\n",
      "[Docker] Preparing\n",
      "[Docker] Preparing\n",
      "[Docker] Preparing\n",
      "[Docker] Preparing\n",
      "[Docker] Preparing\n",
      "[Docker] Preparing\n",
      "[Docker] Preparing\n",
      "[Docker] Preparing\n",
      "[Docker] Preparing\n",
      "[Docker] Preparing\n",
      "[Docker] Preparing\n",
      "[Docker] Preparing\n",
      "[Docker] Preparing\n",
      "[Docker] Preparing\n",
      "[Docker] Preparing\n",
      "[Docker] Preparing\n",
      "[Docker] Preparing\n",
      "[Docker] Preparing\n",
      "[Docker] Preparing\n",
      "[Docker] Preparing\n",
      "[Docker] Preparing\n",
      "[Docker] Preparing\n",
      "[Docker] Preparing\n",
      "[Docker] Preparing\n",
      "[Docker] Preparing\n",
      "[Docker] Preparing\n",
      "[Docker] Preparing\n",
      "[Docker] Preparing\n",
      "[Docker] Preparing\n",
      "[Docker] Preparing\n",
      "[Docker] Preparing\n",
      "[Docker] Preparing\n",
      "[Docker] Preparing\n",
      "[Docker] Preparing\n",
      "[Docker] Preparing\n",
      "[Docker] Waiting\n",
      "[Docker] Waiting\n",
      "[Docker] Waiting\n",
      "[Docker] Waiting\n",
      "[Docker] Waiting\n",
      "[Docker] Waiting\n",
      "[Docker] Waiting\n",
      "[Docker] Waiting\n",
      "[Docker] Waiting\n",
      "[Docker] Waiting\n",
      "[Docker] Waiting\n",
      "[Docker] Waiting\n",
      "[Docker] Waiting\n",
      "[Docker] Waiting\n",
      "[Docker] Waiting\n",
      "[Docker] Waiting\n",
      "[Docker] Waiting\n",
      "[Docker] Waiting\n",
      "[Docker] Waiting\n",
      "[Docker] Waiting\n",
      "[Docker] Waiting\n",
      "[Docker] Waiting\n",
      "[Docker] Waiting\n",
      "[Docker] Waiting\n",
      "[Docker] Waiting\n",
      "[Docker] Waiting\n",
      "[Docker] Waiting\n",
      "[Docker] Waiting\n",
      "[Docker] Waiting\n",
      "[Docker] Waiting\n",
      "[Docker] Waiting\n",
      "[Docker] Waiting\n",
      "[Docker] Waiting\n",
      "[Docker] Waiting\n",
      "[Docker] Waiting\n",
      "[Docker] Waiting\n",
      "[Docker] Pushing\n",
      "[Docker] Layer already exists\n",
      "[Docker] Layer already exists\n",
      "[Docker] Layer already exists\n",
      "[Docker] Layer already exists\n",
      "[Docker] Pushing\n",
      "[Docker] Pushing\n",
      "[Docker] Layer already exists\n",
      "[Docker] Layer already exists\n",
      "[Docker] Layer already exists\n",
      "[Docker] Layer already exists\n",
      "[Docker] Layer already exists\n",
      "[Docker] Layer already exists\n",
      "[Docker] Layer already exists\n",
      "[Docker] Layer already exists\n",
      "[Docker] Layer already exists\n",
      "[Docker] Layer already exists\n",
      "[Docker] Layer already exists\n",
      "[Docker] Layer already exists\n",
      "[Docker] Layer already exists\n",
      "[Docker] Layer already exists\n",
      "[Docker] Layer already exists\n",
      "[Docker] Layer already exists\n",
      "[Docker] Layer already exists\n",
      "[Docker] Layer already exists\n",
      "[Docker] Layer already exists\n",
      "[Docker] Layer already exists\n",
      "[Docker] Layer already exists\n",
      "[Docker] Layer already exists\n",
      "[Docker] Layer already exists\n",
      "[Docker] Layer already exists\n",
      "[Docker] Layer already exists\n",
      "[Docker] Layer already exists\n",
      "[Docker] Layer already exists\n",
      "[Docker] Layer already exists\n",
      "[Docker] Layer already exists\n",
      "[Docker] Layer already exists\n",
      "[Docker] Layer already exists\n",
      "[Docker] Layer already exists\n",
      "[Docker] Layer already exists\n",
      "[Docker] Layer already exists\n",
      "[Docker] Layer already exists\n",
      "[Docker] Layer already exists\n",
      "[Docker] Pushed\n",
      "[Docker] latest: digest: sha256:515cc964d071f34bf1eab43b84489d7fe4c141044444a6ea58d43bf0000a8c1d size: 8926\n",
      "New container image \"gcr.io/astute-pride-317802/data_centric_pipeline@sha256:515cc964d071f34bf1eab43b84489d7fe4c141044444a6ea58d43bf0000a8c1d\" was built.\n",
      "INFO:absl:Generated pipeline:\n",
      " pipeline_info {\n",
      "  id: \"data_centric_pipeline\"\n",
      "}\n",
      "nodes {\n",
      "  pipeline_node {\n",
      "    node_info {\n",
      "      type {\n",
      "        name: \"tfx.components.example_gen.import_example_gen.component.ImportExampleGen\"\n",
      "      }\n",
      "      id: \"ImportExampleGen\"\n",
      "    }\n",
      "    contexts {\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"pipeline\"\n",
      "        }\n",
      "        name {\n",
      "          field_value {\n",
      "            string_value: \"data_centric_pipeline\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"pipeline_run\"\n",
      "        }\n",
      "        name {\n",
      "          runtime_parameter {\n",
      "            name: \"pipeline_run_id\"\n",
      "            type: STRING\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"node\"\n",
      "        }\n",
      "        name {\n",
      "          field_value {\n",
      "            string_value: \"data_centric_pipeline.ImportExampleGen\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    outputs {\n",
      "      outputs {\n",
      "        key: \"examples\"\n",
      "        value {\n",
      "          artifact_spec {\n",
      "            type {\n",
      "              name: \"Examples\"\n",
      "              properties {\n",
      "                key: \"span\"\n",
      "                value: INT\n",
      "              }\n",
      "              properties {\n",
      "                key: \"split_names\"\n",
      "                value: STRING\n",
      "              }\n",
      "              properties {\n",
      "                key: \"version\"\n",
      "                value: INT\n",
      "              }\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    parameters {\n",
      "      parameters {\n",
      "        key: \"input_base\"\n",
      "        value {\n",
      "          field_value {\n",
      "            string_value: \"gs://astute-pride-317802-kubeflowpipelines-default/data-centric/data/\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      parameters {\n",
      "        key: \"input_config\"\n",
      "        value {\n",
      "          field_value {\n",
      "            string_value: \"{\\n  \\\"splits\\\": [\\n    {\\n      \\\"name\\\": \\\"train\\\",\\n      \\\"pattern\\\": \\\"train/train/*\\\"\\n    },\\n    {\\n      \\\"name\\\": \\\"eval\\\",\\n      \\\"pattern\\\": \\\"val/val/*\\\"\\n    }\\n  ]\\n}\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      parameters {\n",
      "        key: \"output_config\"\n",
      "        value {\n",
      "          field_value {\n",
      "            string_value: \"{}\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      parameters {\n",
      "        key: \"output_data_format\"\n",
      "        value {\n",
      "          field_value {\n",
      "            int_value: 6\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    downstream_nodes: \"Evaluator\"\n",
      "    downstream_nodes: \"StatisticsGen\"\n",
      "    downstream_nodes: \"Transform\"\n",
      "    execution_options {\n",
      "      caching_options {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "nodes {\n",
      "  pipeline_node {\n",
      "    node_info {\n",
      "      type {\n",
      "        name: \"tfx.dsl.components.common.resolver.Resolver\"\n",
      "      }\n",
      "      id: \"latest_blessed_model_resolver\"\n",
      "    }\n",
      "    contexts {\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"pipeline\"\n",
      "        }\n",
      "        name {\n",
      "          field_value {\n",
      "            string_value: \"data_centric_pipeline\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"pipeline_run\"\n",
      "        }\n",
      "        name {\n",
      "          runtime_parameter {\n",
      "            name: \"pipeline_run_id\"\n",
      "            type: STRING\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"node\"\n",
      "        }\n",
      "        name {\n",
      "          field_value {\n",
      "            string_value: \"data_centric_pipeline.latest_blessed_model_resolver\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    inputs {\n",
      "      inputs {\n",
      "        key: \"model\"\n",
      "        value {\n",
      "          channels {\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"data_centric_pipeline\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            artifact_query {\n",
      "              type {\n",
      "                name: \"Model\"\n",
      "              }\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      inputs {\n",
      "        key: \"model_blessing\"\n",
      "        value {\n",
      "          channels {\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"data_centric_pipeline\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            artifact_query {\n",
      "              type {\n",
      "                name: \"ModelBlessing\"\n",
      "              }\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      resolver_config {\n",
      "        resolver_steps {\n",
      "          class_path: \"tfx.dsl.input_resolution.strategies.latest_blessed_model_strategy.LatestBlessedModelStrategy\"\n",
      "          config_json: \"{}\"\n",
      "          input_keys: \"model\"\n",
      "          input_keys: \"model_blessing\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    downstream_nodes: \"Evaluator\"\n",
      "    execution_options {\n",
      "      caching_options {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "nodes {\n",
      "  pipeline_node {\n",
      "    node_info {\n",
      "      type {\n",
      "        name: \"tfx.components.statistics_gen.component.StatisticsGen\"\n",
      "      }\n",
      "      id: \"StatisticsGen\"\n",
      "    }\n",
      "    contexts {\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"pipeline\"\n",
      "        }\n",
      "        name {\n",
      "          field_value {\n",
      "            string_value: \"data_centric_pipeline\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"pipeline_run\"\n",
      "        }\n",
      "        name {\n",
      "          runtime_parameter {\n",
      "            name: \"pipeline_run_id\"\n",
      "            type: STRING\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"node\"\n",
      "        }\n",
      "        name {\n",
      "          field_value {\n",
      "            string_value: \"data_centric_pipeline.StatisticsGen\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    inputs {\n",
      "      inputs {\n",
      "        key: \"examples\"\n",
      "        value {\n",
      "          channels {\n",
      "            producer_node_query {\n",
      "              id: \"ImportExampleGen\"\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"data_centric_pipeline\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline_run\"\n",
      "              }\n",
      "              name {\n",
      "                runtime_parameter {\n",
      "                  name: \"pipeline_run_id\"\n",
      "                  type: STRING\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"node\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"data_centric_pipeline.ImportExampleGen\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            artifact_query {\n",
      "              type {\n",
      "                name: \"Examples\"\n",
      "              }\n",
      "            }\n",
      "            output_key: \"examples\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    outputs {\n",
      "      outputs {\n",
      "        key: \"statistics\"\n",
      "        value {\n",
      "          artifact_spec {\n",
      "            type {\n",
      "              name: \"ExampleStatistics\"\n",
      "              properties {\n",
      "                key: \"span\"\n",
      "                value: INT\n",
      "              }\n",
      "              properties {\n",
      "                key: \"split_names\"\n",
      "                value: STRING\n",
      "              }\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    parameters {\n",
      "      parameters {\n",
      "        key: \"exclude_splits\"\n",
      "        value {\n",
      "          field_value {\n",
      "            string_value: \"[]\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    upstream_nodes: \"ImportExampleGen\"\n",
      "    downstream_nodes: \"ExampleValidator\"\n",
      "    downstream_nodes: \"SchemaGen\"\n",
      "    execution_options {\n",
      "      caching_options {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "nodes {\n",
      "  pipeline_node {\n",
      "    node_info {\n",
      "      type {\n",
      "        name: \"tfx.components.schema_gen.component.SchemaGen\"\n",
      "      }\n",
      "      id: \"SchemaGen\"\n",
      "    }\n",
      "    contexts {\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"pipeline\"\n",
      "        }\n",
      "        name {\n",
      "          field_value {\n",
      "            string_value: \"data_centric_pipeline\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"pipeline_run\"\n",
      "        }\n",
      "        name {\n",
      "          runtime_parameter {\n",
      "            name: \"pipeline_run_id\"\n",
      "            type: STRING\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"node\"\n",
      "        }\n",
      "        name {\n",
      "          field_value {\n",
      "            string_value: \"data_centric_pipeline.SchemaGen\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    inputs {\n",
      "      inputs {\n",
      "        key: \"statistics\"\n",
      "        value {\n",
      "          channels {\n",
      "            producer_node_query {\n",
      "              id: \"StatisticsGen\"\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"data_centric_pipeline\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline_run\"\n",
      "              }\n",
      "              name {\n",
      "                runtime_parameter {\n",
      "                  name: \"pipeline_run_id\"\n",
      "                  type: STRING\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"node\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"data_centric_pipeline.StatisticsGen\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            artifact_query {\n",
      "              type {\n",
      "                name: \"ExampleStatistics\"\n",
      "              }\n",
      "            }\n",
      "            output_key: \"statistics\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    outputs {\n",
      "      outputs {\n",
      "        key: \"schema\"\n",
      "        value {\n",
      "          artifact_spec {\n",
      "            type {\n",
      "              name: \"Schema\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    parameters {\n",
      "      parameters {\n",
      "        key: \"exclude_splits\"\n",
      "        value {\n",
      "          field_value {\n",
      "            string_value: \"[]\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      parameters {\n",
      "        key: \"infer_feature_shape\"\n",
      "        value {\n",
      "          field_value {\n",
      "            int_value: 1\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    upstream_nodes: \"StatisticsGen\"\n",
      "    downstream_nodes: \"ExampleValidator\"\n",
      "    downstream_nodes: \"Trainer\"\n",
      "    downstream_nodes: \"Transform\"\n",
      "    execution_options {\n",
      "      caching_options {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "nodes {\n",
      "  pipeline_node {\n",
      "    node_info {\n",
      "      type {\n",
      "        name: \"tfx.components.example_validator.component.ExampleValidator\"\n",
      "      }\n",
      "      id: \"ExampleValidator\"\n",
      "    }\n",
      "    contexts {\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"pipeline\"\n",
      "        }\n",
      "        name {\n",
      "          field_value {\n",
      "            string_value: \"data_centric_pipeline\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"pipeline_run\"\n",
      "        }\n",
      "        name {\n",
      "          runtime_parameter {\n",
      "            name: \"pipeline_run_id\"\n",
      "            type: STRING\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"node\"\n",
      "        }\n",
      "        name {\n",
      "          field_value {\n",
      "            string_value: \"data_centric_pipeline.ExampleValidator\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    inputs {\n",
      "      inputs {\n",
      "        key: \"schema\"\n",
      "        value {\n",
      "          channels {\n",
      "            producer_node_query {\n",
      "              id: \"SchemaGen\"\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"data_centric_pipeline\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline_run\"\n",
      "              }\n",
      "              name {\n",
      "                runtime_parameter {\n",
      "                  name: \"pipeline_run_id\"\n",
      "                  type: STRING\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"node\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"data_centric_pipeline.SchemaGen\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            artifact_query {\n",
      "              type {\n",
      "                name: \"Schema\"\n",
      "              }\n",
      "            }\n",
      "            output_key: \"schema\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      inputs {\n",
      "        key: \"statistics\"\n",
      "        value {\n",
      "          channels {\n",
      "            producer_node_query {\n",
      "              id: \"StatisticsGen\"\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"data_centric_pipeline\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline_run\"\n",
      "              }\n",
      "              name {\n",
      "                runtime_parameter {\n",
      "                  name: \"pipeline_run_id\"\n",
      "                  type: STRING\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"node\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"data_centric_pipeline.StatisticsGen\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            artifact_query {\n",
      "              type {\n",
      "                name: \"ExampleStatistics\"\n",
      "              }\n",
      "            }\n",
      "            output_key: \"statistics\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    outputs {\n",
      "      outputs {\n",
      "        key: \"anomalies\"\n",
      "        value {\n",
      "          artifact_spec {\n",
      "            type {\n",
      "              name: \"ExampleAnomalies\"\n",
      "              properties {\n",
      "                key: \"span\"\n",
      "                value: INT\n",
      "              }\n",
      "              properties {\n",
      "                key: \"split_names\"\n",
      "                value: STRING\n",
      "              }\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    parameters {\n",
      "      parameters {\n",
      "        key: \"exclude_splits\"\n",
      "        value {\n",
      "          field_value {\n",
      "            string_value: \"[]\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    upstream_nodes: \"SchemaGen\"\n",
      "    upstream_nodes: \"StatisticsGen\"\n",
      "    execution_options {\n",
      "      caching_options {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "nodes {\n",
      "  pipeline_node {\n",
      "    node_info {\n",
      "      type {\n",
      "        name: \"tfx.components.transform.component.Transform\"\n",
      "      }\n",
      "      id: \"Transform\"\n",
      "    }\n",
      "    contexts {\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"pipeline\"\n",
      "        }\n",
      "        name {\n",
      "          field_value {\n",
      "            string_value: \"data_centric_pipeline\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"pipeline_run\"\n",
      "        }\n",
      "        name {\n",
      "          runtime_parameter {\n",
      "            name: \"pipeline_run_id\"\n",
      "            type: STRING\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"node\"\n",
      "        }\n",
      "        name {\n",
      "          field_value {\n",
      "            string_value: \"data_centric_pipeline.Transform\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    inputs {\n",
      "      inputs {\n",
      "        key: \"examples\"\n",
      "        value {\n",
      "          channels {\n",
      "            producer_node_query {\n",
      "              id: \"ImportExampleGen\"\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"data_centric_pipeline\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline_run\"\n",
      "              }\n",
      "              name {\n",
      "                runtime_parameter {\n",
      "                  name: \"pipeline_run_id\"\n",
      "                  type: STRING\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"node\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"data_centric_pipeline.ImportExampleGen\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            artifact_query {\n",
      "              type {\n",
      "                name: \"Examples\"\n",
      "              }\n",
      "            }\n",
      "            output_key: \"examples\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      inputs {\n",
      "        key: \"schema\"\n",
      "        value {\n",
      "          channels {\n",
      "            producer_node_query {\n",
      "              id: \"SchemaGen\"\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"data_centric_pipeline\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline_run\"\n",
      "              }\n",
      "              name {\n",
      "                runtime_parameter {\n",
      "                  name: \"pipeline_run_id\"\n",
      "                  type: STRING\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"node\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"data_centric_pipeline.SchemaGen\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            artifact_query {\n",
      "              type {\n",
      "                name: \"Schema\"\n",
      "              }\n",
      "            }\n",
      "            output_key: \"schema\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    outputs {\n",
      "      outputs {\n",
      "        key: \"transform_graph\"\n",
      "        value {\n",
      "          artifact_spec {\n",
      "            type {\n",
      "              name: \"TransformGraph\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      outputs {\n",
      "        key: \"transformed_examples\"\n",
      "        value {\n",
      "          artifact_spec {\n",
      "            type {\n",
      "              name: \"Examples\"\n",
      "              properties {\n",
      "                key: \"span\"\n",
      "                value: INT\n",
      "              }\n",
      "              properties {\n",
      "                key: \"split_names\"\n",
      "                value: STRING\n",
      "              }\n",
      "              properties {\n",
      "                key: \"version\"\n",
      "                value: INT\n",
      "              }\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      outputs {\n",
      "        key: \"updated_analyzer_cache\"\n",
      "        value {\n",
      "          artifact_spec {\n",
      "            type {\n",
      "              name: \"TransformCache\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    parameters {\n",
      "      parameters {\n",
      "        key: \"custom_config\"\n",
      "        value {\n",
      "          field_value {\n",
      "            string_value: \"null\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      parameters {\n",
      "        key: \"force_tf_compat_v1\"\n",
      "        value {\n",
      "          field_value {\n",
      "            int_value: 0\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      parameters {\n",
      "        key: \"preprocessing_fn\"\n",
      "        value {\n",
      "          field_value {\n",
      "            string_value: \"models.preprocessing.preprocessing_fn\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    upstream_nodes: \"ImportExampleGen\"\n",
      "    upstream_nodes: \"SchemaGen\"\n",
      "    downstream_nodes: \"Trainer\"\n",
      "    execution_options {\n",
      "      caching_options {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "nodes {\n",
      "  pipeline_node {\n",
      "    node_info {\n",
      "      type {\n",
      "        name: \"tfx.components.trainer.component.Trainer\"\n",
      "      }\n",
      "      id: \"Trainer\"\n",
      "    }\n",
      "    contexts {\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"pipeline\"\n",
      "        }\n",
      "        name {\n",
      "          field_value {\n",
      "            string_value: \"data_centric_pipeline\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"pipeline_run\"\n",
      "        }\n",
      "        name {\n",
      "          runtime_parameter {\n",
      "            name: \"pipeline_run_id\"\n",
      "            type: STRING\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"node\"\n",
      "        }\n",
      "        name {\n",
      "          field_value {\n",
      "            string_value: \"data_centric_pipeline.Trainer\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    inputs {\n",
      "      inputs {\n",
      "        key: \"examples\"\n",
      "        value {\n",
      "          channels {\n",
      "            producer_node_query {\n",
      "              id: \"Transform\"\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"data_centric_pipeline\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline_run\"\n",
      "              }\n",
      "              name {\n",
      "                runtime_parameter {\n",
      "                  name: \"pipeline_run_id\"\n",
      "                  type: STRING\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"node\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"data_centric_pipeline.Transform\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            artifact_query {\n",
      "              type {\n",
      "                name: \"Examples\"\n",
      "              }\n",
      "            }\n",
      "            output_key: \"transformed_examples\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      inputs {\n",
      "        key: \"schema\"\n",
      "        value {\n",
      "          channels {\n",
      "            producer_node_query {\n",
      "              id: \"SchemaGen\"\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"data_centric_pipeline\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline_run\"\n",
      "              }\n",
      "              name {\n",
      "                runtime_parameter {\n",
      "                  name: \"pipeline_run_id\"\n",
      "                  type: STRING\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"node\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"data_centric_pipeline.SchemaGen\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            artifact_query {\n",
      "              type {\n",
      "                name: \"Schema\"\n",
      "              }\n",
      "            }\n",
      "            output_key: \"schema\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      inputs {\n",
      "        key: \"transform_graph\"\n",
      "        value {\n",
      "          channels {\n",
      "            producer_node_query {\n",
      "              id: \"Transform\"\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"data_centric_pipeline\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline_run\"\n",
      "              }\n",
      "              name {\n",
      "                runtime_parameter {\n",
      "                  name: \"pipeline_run_id\"\n",
      "                  type: STRING\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"node\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"data_centric_pipeline.Transform\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            artifact_query {\n",
      "              type {\n",
      "                name: \"TransformGraph\"\n",
      "              }\n",
      "            }\n",
      "            output_key: \"transform_graph\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    outputs {\n",
      "      outputs {\n",
      "        key: \"model\"\n",
      "        value {\n",
      "          artifact_spec {\n",
      "            type {\n",
      "              name: \"Model\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      outputs {\n",
      "        key: \"model_run\"\n",
      "        value {\n",
      "          artifact_spec {\n",
      "            type {\n",
      "              name: \"ModelRun\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    parameters {\n",
      "      parameters {\n",
      "        key: \"custom_config\"\n",
      "        value {\n",
      "          field_value {\n",
      "            string_value: \"null\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      parameters {\n",
      "        key: \"eval_args\"\n",
      "        value {\n",
      "          field_value {\n",
      "            string_value: \"{\\n  \\\"num_steps\\\": 50\\n}\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      parameters {\n",
      "        key: \"run_fn\"\n",
      "        value {\n",
      "          field_value {\n",
      "            string_value: \"models.keras.model.run_fn\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      parameters {\n",
      "        key: \"train_args\"\n",
      "        value {\n",
      "          field_value {\n",
      "            string_value: \"{\\n  \\\"num_steps\\\": 100\\n}\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    upstream_nodes: \"SchemaGen\"\n",
      "    upstream_nodes: \"Transform\"\n",
      "    downstream_nodes: \"Evaluator\"\n",
      "    execution_options {\n",
      "      caching_options {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "nodes {\n",
      "  pipeline_node {\n",
      "    node_info {\n",
      "      type {\n",
      "        name: \"tfx.components.evaluator.component.Evaluator\"\n",
      "      }\n",
      "      id: \"Evaluator\"\n",
      "    }\n",
      "    contexts {\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"pipeline\"\n",
      "        }\n",
      "        name {\n",
      "          field_value {\n",
      "            string_value: \"data_centric_pipeline\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"pipeline_run\"\n",
      "        }\n",
      "        name {\n",
      "          runtime_parameter {\n",
      "            name: \"pipeline_run_id\"\n",
      "            type: STRING\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"node\"\n",
      "        }\n",
      "        name {\n",
      "          field_value {\n",
      "            string_value: \"data_centric_pipeline.Evaluator\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    inputs {\n",
      "      inputs {\n",
      "        key: \"baseline_model\"\n",
      "        value {\n",
      "          channels {\n",
      "            producer_node_query {\n",
      "              id: \"latest_blessed_model_resolver\"\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"data_centric_pipeline\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline_run\"\n",
      "              }\n",
      "              name {\n",
      "                runtime_parameter {\n",
      "                  name: \"pipeline_run_id\"\n",
      "                  type: STRING\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"node\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"data_centric_pipeline.latest_blessed_model_resolver\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            artifact_query {\n",
      "              type {\n",
      "                name: \"Model\"\n",
      "              }\n",
      "            }\n",
      "            output_key: \"model\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      inputs {\n",
      "        key: \"examples\"\n",
      "        value {\n",
      "          channels {\n",
      "            producer_node_query {\n",
      "              id: \"ImportExampleGen\"\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"data_centric_pipeline\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline_run\"\n",
      "              }\n",
      "              name {\n",
      "                runtime_parameter {\n",
      "                  name: \"pipeline_run_id\"\n",
      "                  type: STRING\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"node\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"data_centric_pipeline.ImportExampleGen\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            artifact_query {\n",
      "              type {\n",
      "                name: \"Examples\"\n",
      "              }\n",
      "            }\n",
      "            output_key: \"examples\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      inputs {\n",
      "        key: \"model\"\n",
      "        value {\n",
      "          channels {\n",
      "            producer_node_query {\n",
      "              id: \"Trainer\"\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"data_centric_pipeline\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline_run\"\n",
      "              }\n",
      "              name {\n",
      "                runtime_parameter {\n",
      "                  name: \"pipeline_run_id\"\n",
      "                  type: STRING\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"node\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"data_centric_pipeline.Trainer\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            artifact_query {\n",
      "              type {\n",
      "                name: \"Model\"\n",
      "              }\n",
      "            }\n",
      "            output_key: \"model\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    outputs {\n",
      "      outputs {\n",
      "        key: \"blessing\"\n",
      "        value {\n",
      "          artifact_spec {\n",
      "            type {\n",
      "              name: \"ModelBlessing\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      outputs {\n",
      "        key: \"evaluation\"\n",
      "        value {\n",
      "          artifact_spec {\n",
      "            type {\n",
      "              name: \"ModelEvaluation\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    parameters {\n",
      "      parameters {\n",
      "        key: \"eval_config\"\n",
      "        value {\n",
      "          field_value {\n",
      "            string_value: \"{\\n  \\\"metrics_specs\\\": [\\n    {\\n      \\\"metrics\\\": [\\n        {\\n          \\\"class_name\\\": \\\"BinaryAccuracy\\\",\\n          \\\"threshold\\\": {\\n            \\\"change_threshold\\\": {\\n              \\\"absolute\\\": -1e-10,\\n              \\\"direction\\\": \\\"HIGHER_IS_BETTER\\\"\\n            },\\n            \\\"value_threshold\\\": {\\n              \\\"lower_bound\\\": 0.6\\n            }\\n          }\\n        }\\n      ]\\n    }\\n  ],\\n  \\\"model_specs\\\": [\\n    {\\n      \\\"label_key\\\": \\\"label\\\"\\n    }\\n  ],\\n  \\\"slicing_specs\\\": [\\n    {}\\n  ]\\n}\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      parameters {\n",
      "        key: \"example_splits\"\n",
      "        value {\n",
      "          field_value {\n",
      "            string_value: \"null\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    upstream_nodes: \"ImportExampleGen\"\n",
      "    upstream_nodes: \"Trainer\"\n",
      "    upstream_nodes: \"latest_blessed_model_resolver\"\n",
      "    execution_options {\n",
      "      caching_options {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "runtime_spec {\n",
      "  pipeline_root {\n",
      "    runtime_parameter {\n",
      "      name: \"pipeline_root\"\n",
      "      type: STRING\n",
      "      default_value {\n",
      "        string_value: \"gs://astute-pride-317802-kubeflowpipelines-default/tfx_pipeline_output/data_centric_pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  pipeline_run_id {\n",
      "    runtime_parameter {\n",
      "      name: \"pipeline_run_id\"\n",
      "      type: STRING\n",
      "    }\n",
      "  }\n",
      "}\n",
      "execution_mode: SYNC\n",
      "deployment_config {\n",
      "  type_url: \"type.googleapis.com/tfx.orchestration.IntermediateDeploymentConfig\"\n",
      "  value: \"\\n\\220\\001\\n\\rStatisticsGen\\022\\177\\nHtype.googleapis.com/tfx.orchestration.executable_spec.BeamExecutableSpec\\0223\\n1\\n/tfx.components.statistics_gen.executor.Executor\\n\\216\\001\\n\\tSchemaGen\\022\\200\\001\\nOtype.googleapis.com/tfx.orchestration.executable_spec.PythonClassExecutableSpec\\022-\\n+tfx.components.schema_gen.executor.Executor\\n\\220\\001\\n\\007Trainer\\022\\204\\001\\nOtype.googleapis.com/tfx.orchestration.executable_spec.PythonClassExecutableSpec\\0221\\n/tfx.components.trainer.executor.GenericExecutor\\n\\244\\001\\n\\020ImportExampleGen\\022\\217\\001\\nHtype.googleapis.com/tfx.orchestration.executable_spec.BeamExecutableSpec\\022C\\nA\\n?tfx.components.example_gen.import_example_gen.executor.Executor\\n\\207\\001\\n\\tTransform\\022z\\nHtype.googleapis.com/tfx.orchestration.executable_spec.BeamExecutableSpec\\022.\\n,\\n*tfx.components.transform.executor.Executor\\n\\234\\001\\n\\020ExampleValidator\\022\\207\\001\\nOtype.googleapis.com/tfx.orchestration.executable_spec.PythonClassExecutableSpec\\0224\\n2tfx.components.example_validator.executor.Executor\\n\\207\\001\\n\\tEvaluator\\022z\\nHtype.googleapis.com/tfx.orchestration.executable_spec.BeamExecutableSpec\\022.\\n,\\n*tfx.components.evaluator.executor.Executor\\022\\233\\001\\n\\020ImportExampleGen\\022\\206\\001\\nOtype.googleapis.com/tfx.orchestration.executable_spec.PythonClassExecutableSpec\\0223\\n1tfx.components.example_gen.driver.FileBasedDriver\"\n",
      "}\n",
      "\n",
      "INFO:absl:Adding upstream dependencies for component importexamplegen\n",
      "INFO:absl:Adding upstream dependencies for component latest-blessed-model-resolver\n",
      "INFO:absl:Adding upstream dependencies for component statisticsgen\n",
      "INFO:absl:   ->  Component: importexamplegen\n",
      "INFO:absl:Adding upstream dependencies for component schemagen\n",
      "INFO:absl:   ->  Component: statisticsgen\n",
      "INFO:absl:Adding upstream dependencies for component examplevalidator\n",
      "INFO:absl:   ->  Component: schemagen\n",
      "INFO:absl:   ->  Component: statisticsgen\n",
      "INFO:absl:Adding upstream dependencies for component transform\n",
      "INFO:absl:   ->  Component: schemagen\n",
      "INFO:absl:   ->  Component: importexamplegen\n",
      "INFO:absl:Adding upstream dependencies for component trainer\n",
      "INFO:absl:   ->  Component: schemagen\n",
      "INFO:absl:   ->  Component: transform\n",
      "INFO:absl:Adding upstream dependencies for component evaluator\n",
      "INFO:absl:   ->  Component: trainer\n",
      "INFO:absl:   ->  Component: latest-blessed-model-resolver\n",
      "INFO:absl:   ->  Component: importexamplegen\n",
      "Pipeline \"data_centric_pipeline\" already exists. id=\"486abe31-ff87-43b1-b8db-7c477e73f4ad\")\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!tfx pipeline create  --pipeline-path=kubeflow_runner.py --endpoint={ENDPOINT} \\\n",
    "--build-image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Pipeline\n",
    "check kubeflow dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "cKSjVVsa0oli"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-26 02:13:34.277943: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "WARNING:absl:RuntimeParameter is only supported on Cloud-based DAG runner currently.\n",
      "CLI\n",
      "Creating a run for pipeline: data_centric_pipeline\n",
      "Detected Kubeflow.\n",
      "Use --engine flag if you intend to use a different orchestrator.\n",
      "/opt/conda/lib/python3.7/site-packages/kfp/_client.py:182: UserWarning: The host 314cd0b9ca67984e-dot-asia-east1.pipelines.googleusercontent.com does not contain the \"http\" or \"https\" protocol. Defaults to \"https\".\n",
      "  ' Defaults to \"https\".' % host)\n",
      "Run created for pipeline: data_centric_pipeline\n",
      "+=======================+======================================+========+===========================+============================================================================================================================+\n",
      "| pipeline_name         | run_id                               | status | created_at                | link                                                                                                                       |\n",
      "+=======================+======================================+========+===========================+============================================================================================================================+\n",
      "| data_centric_pipeline | 7c5313d4-bd94-43c8-885d-d9afc2db34f5 | None   | 2021-06-26T02:13:39+00:00 | http://314cd0b9ca67984e-dot-asia-east1.pipelines.googleusercontent.com/#/runs/details/7c5313d4-bd94-43c8-885d-d9afc2db34f5 |\n",
      "+=======================+======================================+========+===========================+============================================================================================================================+\n",
      "\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!tfx run create --pipeline-name={PIPELINE_NAME} --endpoint={ENDPOINT}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "VE-Pqvto0olm",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-06-26 04:01:58.266042: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "WARNING:absl:RuntimeParameter is only supported on Cloud-based DAG runner currently.\n",
      "CLI\n",
      "Updating pipeline\n",
      "Detected Kubeflow.\n",
      "Use --engine flag if you intend to use a different orchestrator.\n",
      "/opt/conda/lib/python3.7/site-packages/kfp/_client.py:182: UserWarning: The host 314cd0b9ca67984e-dot-asia-east1.pipelines.googleusercontent.com does not contain the \"http\" or \"https\" protocol. Defaults to \"https\".\n",
      "  ' Defaults to \"https\".' % host)\n",
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "INFO:absl:Excluding no splits because exclude_splits is not set.\n",
      "INFO:absl:Generated pipeline:\n",
      " pipeline_info {\n",
      "  id: \"data_centric_pipeline\"\n",
      "}\n",
      "nodes {\n",
      "  pipeline_node {\n",
      "    node_info {\n",
      "      type {\n",
      "        name: \"tfx.components.example_gen.import_example_gen.component.ImportExampleGen\"\n",
      "      }\n",
      "      id: \"ImportExampleGen\"\n",
      "    }\n",
      "    contexts {\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"pipeline\"\n",
      "        }\n",
      "        name {\n",
      "          field_value {\n",
      "            string_value: \"data_centric_pipeline\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"pipeline_run\"\n",
      "        }\n",
      "        name {\n",
      "          runtime_parameter {\n",
      "            name: \"pipeline_run_id\"\n",
      "            type: STRING\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"node\"\n",
      "        }\n",
      "        name {\n",
      "          field_value {\n",
      "            string_value: \"data_centric_pipeline.ImportExampleGen\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    outputs {\n",
      "      outputs {\n",
      "        key: \"examples\"\n",
      "        value {\n",
      "          artifact_spec {\n",
      "            type {\n",
      "              name: \"Examples\"\n",
      "              properties {\n",
      "                key: \"span\"\n",
      "                value: INT\n",
      "              }\n",
      "              properties {\n",
      "                key: \"split_names\"\n",
      "                value: STRING\n",
      "              }\n",
      "              properties {\n",
      "                key: \"version\"\n",
      "                value: INT\n",
      "              }\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    parameters {\n",
      "      parameters {\n",
      "        key: \"input_base\"\n",
      "        value {\n",
      "          field_value {\n",
      "            string_value: \"gs://astute-pride-317802-kubeflowpipelines-default/data-centric/data/\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      parameters {\n",
      "        key: \"input_config\"\n",
      "        value {\n",
      "          field_value {\n",
      "            string_value: \"{\\n  \\\"splits\\\": [\\n    {\\n      \\\"name\\\": \\\"train\\\",\\n      \\\"pattern\\\": \\\"train/train/*\\\"\\n    },\\n    {\\n      \\\"name\\\": \\\"eval\\\",\\n      \\\"pattern\\\": \\\"val/val/*\\\"\\n    }\\n  ]\\n}\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      parameters {\n",
      "        key: \"output_config\"\n",
      "        value {\n",
      "          field_value {\n",
      "            string_value: \"{}\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      parameters {\n",
      "        key: \"output_data_format\"\n",
      "        value {\n",
      "          field_value {\n",
      "            int_value: 6\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    downstream_nodes: \"StatisticsGen\"\n",
      "    downstream_nodes: \"Transform\"\n",
      "    execution_options {\n",
      "      caching_options {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "nodes {\n",
      "  pipeline_node {\n",
      "    node_info {\n",
      "      type {\n",
      "        name: \"tfx.dsl.components.common.resolver.Resolver\"\n",
      "      }\n",
      "      id: \"latest_blessed_model_resolver\"\n",
      "    }\n",
      "    contexts {\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"pipeline\"\n",
      "        }\n",
      "        name {\n",
      "          field_value {\n",
      "            string_value: \"data_centric_pipeline\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"pipeline_run\"\n",
      "        }\n",
      "        name {\n",
      "          runtime_parameter {\n",
      "            name: \"pipeline_run_id\"\n",
      "            type: STRING\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"node\"\n",
      "        }\n",
      "        name {\n",
      "          field_value {\n",
      "            string_value: \"data_centric_pipeline.latest_blessed_model_resolver\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    inputs {\n",
      "      inputs {\n",
      "        key: \"model\"\n",
      "        value {\n",
      "          channels {\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"data_centric_pipeline\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            artifact_query {\n",
      "              type {\n",
      "                name: \"Model\"\n",
      "              }\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      inputs {\n",
      "        key: \"model_blessing\"\n",
      "        value {\n",
      "          channels {\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"data_centric_pipeline\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            artifact_query {\n",
      "              type {\n",
      "                name: \"ModelBlessing\"\n",
      "              }\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      resolver_config {\n",
      "        resolver_steps {\n",
      "          class_path: \"tfx.dsl.input_resolution.strategies.latest_blessed_model_strategy.LatestBlessedModelStrategy\"\n",
      "          config_json: \"{}\"\n",
      "          input_keys: \"model\"\n",
      "          input_keys: \"model_blessing\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    execution_options {\n",
      "      caching_options {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "nodes {\n",
      "  pipeline_node {\n",
      "    node_info {\n",
      "      type {\n",
      "        name: \"tfx.components.statistics_gen.component.StatisticsGen\"\n",
      "      }\n",
      "      id: \"StatisticsGen\"\n",
      "    }\n",
      "    contexts {\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"pipeline\"\n",
      "        }\n",
      "        name {\n",
      "          field_value {\n",
      "            string_value: \"data_centric_pipeline\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"pipeline_run\"\n",
      "        }\n",
      "        name {\n",
      "          runtime_parameter {\n",
      "            name: \"pipeline_run_id\"\n",
      "            type: STRING\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"node\"\n",
      "        }\n",
      "        name {\n",
      "          field_value {\n",
      "            string_value: \"data_centric_pipeline.StatisticsGen\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    inputs {\n",
      "      inputs {\n",
      "        key: \"examples\"\n",
      "        value {\n",
      "          channels {\n",
      "            producer_node_query {\n",
      "              id: \"ImportExampleGen\"\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"data_centric_pipeline\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline_run\"\n",
      "              }\n",
      "              name {\n",
      "                runtime_parameter {\n",
      "                  name: \"pipeline_run_id\"\n",
      "                  type: STRING\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"node\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"data_centric_pipeline.ImportExampleGen\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            artifact_query {\n",
      "              type {\n",
      "                name: \"Examples\"\n",
      "              }\n",
      "            }\n",
      "            output_key: \"examples\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    outputs {\n",
      "      outputs {\n",
      "        key: \"statistics\"\n",
      "        value {\n",
      "          artifact_spec {\n",
      "            type {\n",
      "              name: \"ExampleStatistics\"\n",
      "              properties {\n",
      "                key: \"span\"\n",
      "                value: INT\n",
      "              }\n",
      "              properties {\n",
      "                key: \"split_names\"\n",
      "                value: STRING\n",
      "              }\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    parameters {\n",
      "      parameters {\n",
      "        key: \"exclude_splits\"\n",
      "        value {\n",
      "          field_value {\n",
      "            string_value: \"[]\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    upstream_nodes: \"ImportExampleGen\"\n",
      "    downstream_nodes: \"ExampleValidator\"\n",
      "    downstream_nodes: \"SchemaGen\"\n",
      "    execution_options {\n",
      "      caching_options {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "nodes {\n",
      "  pipeline_node {\n",
      "    node_info {\n",
      "      type {\n",
      "        name: \"tfx.components.schema_gen.component.SchemaGen\"\n",
      "      }\n",
      "      id: \"SchemaGen\"\n",
      "    }\n",
      "    contexts {\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"pipeline\"\n",
      "        }\n",
      "        name {\n",
      "          field_value {\n",
      "            string_value: \"data_centric_pipeline\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"pipeline_run\"\n",
      "        }\n",
      "        name {\n",
      "          runtime_parameter {\n",
      "            name: \"pipeline_run_id\"\n",
      "            type: STRING\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"node\"\n",
      "        }\n",
      "        name {\n",
      "          field_value {\n",
      "            string_value: \"data_centric_pipeline.SchemaGen\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    inputs {\n",
      "      inputs {\n",
      "        key: \"statistics\"\n",
      "        value {\n",
      "          channels {\n",
      "            producer_node_query {\n",
      "              id: \"StatisticsGen\"\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"data_centric_pipeline\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline_run\"\n",
      "              }\n",
      "              name {\n",
      "                runtime_parameter {\n",
      "                  name: \"pipeline_run_id\"\n",
      "                  type: STRING\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"node\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"data_centric_pipeline.StatisticsGen\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            artifact_query {\n",
      "              type {\n",
      "                name: \"ExampleStatistics\"\n",
      "              }\n",
      "            }\n",
      "            output_key: \"statistics\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    outputs {\n",
      "      outputs {\n",
      "        key: \"schema\"\n",
      "        value {\n",
      "          artifact_spec {\n",
      "            type {\n",
      "              name: \"Schema\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    parameters {\n",
      "      parameters {\n",
      "        key: \"exclude_splits\"\n",
      "        value {\n",
      "          field_value {\n",
      "            string_value: \"[]\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      parameters {\n",
      "        key: \"infer_feature_shape\"\n",
      "        value {\n",
      "          field_value {\n",
      "            int_value: 1\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    upstream_nodes: \"StatisticsGen\"\n",
      "    downstream_nodes: \"ExampleValidator\"\n",
      "    downstream_nodes: \"Trainer\"\n",
      "    downstream_nodes: \"Transform\"\n",
      "    execution_options {\n",
      "      caching_options {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "nodes {\n",
      "  pipeline_node {\n",
      "    node_info {\n",
      "      type {\n",
      "        name: \"tfx.components.example_validator.component.ExampleValidator\"\n",
      "      }\n",
      "      id: \"ExampleValidator\"\n",
      "    }\n",
      "    contexts {\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"pipeline\"\n",
      "        }\n",
      "        name {\n",
      "          field_value {\n",
      "            string_value: \"data_centric_pipeline\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"pipeline_run\"\n",
      "        }\n",
      "        name {\n",
      "          runtime_parameter {\n",
      "            name: \"pipeline_run_id\"\n",
      "            type: STRING\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"node\"\n",
      "        }\n",
      "        name {\n",
      "          field_value {\n",
      "            string_value: \"data_centric_pipeline.ExampleValidator\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    inputs {\n",
      "      inputs {\n",
      "        key: \"schema\"\n",
      "        value {\n",
      "          channels {\n",
      "            producer_node_query {\n",
      "              id: \"SchemaGen\"\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"data_centric_pipeline\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline_run\"\n",
      "              }\n",
      "              name {\n",
      "                runtime_parameter {\n",
      "                  name: \"pipeline_run_id\"\n",
      "                  type: STRING\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"node\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"data_centric_pipeline.SchemaGen\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            artifact_query {\n",
      "              type {\n",
      "                name: \"Schema\"\n",
      "              }\n",
      "            }\n",
      "            output_key: \"schema\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      inputs {\n",
      "        key: \"statistics\"\n",
      "        value {\n",
      "          channels {\n",
      "            producer_node_query {\n",
      "              id: \"StatisticsGen\"\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"data_centric_pipeline\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline_run\"\n",
      "              }\n",
      "              name {\n",
      "                runtime_parameter {\n",
      "                  name: \"pipeline_run_id\"\n",
      "                  type: STRING\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"node\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"data_centric_pipeline.StatisticsGen\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            artifact_query {\n",
      "              type {\n",
      "                name: \"ExampleStatistics\"\n",
      "              }\n",
      "            }\n",
      "            output_key: \"statistics\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    outputs {\n",
      "      outputs {\n",
      "        key: \"anomalies\"\n",
      "        value {\n",
      "          artifact_spec {\n",
      "            type {\n",
      "              name: \"ExampleAnomalies\"\n",
      "              properties {\n",
      "                key: \"span\"\n",
      "                value: INT\n",
      "              }\n",
      "              properties {\n",
      "                key: \"split_names\"\n",
      "                value: STRING\n",
      "              }\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    parameters {\n",
      "      parameters {\n",
      "        key: \"exclude_splits\"\n",
      "        value {\n",
      "          field_value {\n",
      "            string_value: \"[]\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    upstream_nodes: \"SchemaGen\"\n",
      "    upstream_nodes: \"StatisticsGen\"\n",
      "    execution_options {\n",
      "      caching_options {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "nodes {\n",
      "  pipeline_node {\n",
      "    node_info {\n",
      "      type {\n",
      "        name: \"tfx.components.transform.component.Transform\"\n",
      "      }\n",
      "      id: \"Transform\"\n",
      "    }\n",
      "    contexts {\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"pipeline\"\n",
      "        }\n",
      "        name {\n",
      "          field_value {\n",
      "            string_value: \"data_centric_pipeline\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"pipeline_run\"\n",
      "        }\n",
      "        name {\n",
      "          runtime_parameter {\n",
      "            name: \"pipeline_run_id\"\n",
      "            type: STRING\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"node\"\n",
      "        }\n",
      "        name {\n",
      "          field_value {\n",
      "            string_value: \"data_centric_pipeline.Transform\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    inputs {\n",
      "      inputs {\n",
      "        key: \"examples\"\n",
      "        value {\n",
      "          channels {\n",
      "            producer_node_query {\n",
      "              id: \"ImportExampleGen\"\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"data_centric_pipeline\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline_run\"\n",
      "              }\n",
      "              name {\n",
      "                runtime_parameter {\n",
      "                  name: \"pipeline_run_id\"\n",
      "                  type: STRING\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"node\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"data_centric_pipeline.ImportExampleGen\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            artifact_query {\n",
      "              type {\n",
      "                name: \"Examples\"\n",
      "              }\n",
      "            }\n",
      "            output_key: \"examples\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      inputs {\n",
      "        key: \"schema\"\n",
      "        value {\n",
      "          channels {\n",
      "            producer_node_query {\n",
      "              id: \"SchemaGen\"\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"data_centric_pipeline\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline_run\"\n",
      "              }\n",
      "              name {\n",
      "                runtime_parameter {\n",
      "                  name: \"pipeline_run_id\"\n",
      "                  type: STRING\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"node\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"data_centric_pipeline.SchemaGen\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            artifact_query {\n",
      "              type {\n",
      "                name: \"Schema\"\n",
      "              }\n",
      "            }\n",
      "            output_key: \"schema\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    outputs {\n",
      "      outputs {\n",
      "        key: \"transform_graph\"\n",
      "        value {\n",
      "          artifact_spec {\n",
      "            type {\n",
      "              name: \"TransformGraph\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      outputs {\n",
      "        key: \"transformed_examples\"\n",
      "        value {\n",
      "          artifact_spec {\n",
      "            type {\n",
      "              name: \"Examples\"\n",
      "              properties {\n",
      "                key: \"span\"\n",
      "                value: INT\n",
      "              }\n",
      "              properties {\n",
      "                key: \"split_names\"\n",
      "                value: STRING\n",
      "              }\n",
      "              properties {\n",
      "                key: \"version\"\n",
      "                value: INT\n",
      "              }\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      outputs {\n",
      "        key: \"updated_analyzer_cache\"\n",
      "        value {\n",
      "          artifact_spec {\n",
      "            type {\n",
      "              name: \"TransformCache\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    parameters {\n",
      "      parameters {\n",
      "        key: \"custom_config\"\n",
      "        value {\n",
      "          field_value {\n",
      "            string_value: \"null\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      parameters {\n",
      "        key: \"force_tf_compat_v1\"\n",
      "        value {\n",
      "          field_value {\n",
      "            int_value: 0\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      parameters {\n",
      "        key: \"preprocessing_fn\"\n",
      "        value {\n",
      "          field_value {\n",
      "            string_value: \"models.preprocessing.preprocessing_fn\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    upstream_nodes: \"ImportExampleGen\"\n",
      "    upstream_nodes: \"SchemaGen\"\n",
      "    downstream_nodes: \"Trainer\"\n",
      "    execution_options {\n",
      "      caching_options {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "nodes {\n",
      "  pipeline_node {\n",
      "    node_info {\n",
      "      type {\n",
      "        name: \"tfx.components.trainer.component.Trainer\"\n",
      "      }\n",
      "      id: \"Trainer\"\n",
      "    }\n",
      "    contexts {\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"pipeline\"\n",
      "        }\n",
      "        name {\n",
      "          field_value {\n",
      "            string_value: \"data_centric_pipeline\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"pipeline_run\"\n",
      "        }\n",
      "        name {\n",
      "          runtime_parameter {\n",
      "            name: \"pipeline_run_id\"\n",
      "            type: STRING\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      contexts {\n",
      "        type {\n",
      "          name: \"node\"\n",
      "        }\n",
      "        name {\n",
      "          field_value {\n",
      "            string_value: \"data_centric_pipeline.Trainer\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    inputs {\n",
      "      inputs {\n",
      "        key: \"examples\"\n",
      "        value {\n",
      "          channels {\n",
      "            producer_node_query {\n",
      "              id: \"Transform\"\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"data_centric_pipeline\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline_run\"\n",
      "              }\n",
      "              name {\n",
      "                runtime_parameter {\n",
      "                  name: \"pipeline_run_id\"\n",
      "                  type: STRING\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"node\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"data_centric_pipeline.Transform\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            artifact_query {\n",
      "              type {\n",
      "                name: \"Examples\"\n",
      "              }\n",
      "            }\n",
      "            output_key: \"transformed_examples\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      inputs {\n",
      "        key: \"schema\"\n",
      "        value {\n",
      "          channels {\n",
      "            producer_node_query {\n",
      "              id: \"SchemaGen\"\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"data_centric_pipeline\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline_run\"\n",
      "              }\n",
      "              name {\n",
      "                runtime_parameter {\n",
      "                  name: \"pipeline_run_id\"\n",
      "                  type: STRING\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"node\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"data_centric_pipeline.SchemaGen\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            artifact_query {\n",
      "              type {\n",
      "                name: \"Schema\"\n",
      "              }\n",
      "            }\n",
      "            output_key: \"schema\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      inputs {\n",
      "        key: \"transform_graph\"\n",
      "        value {\n",
      "          channels {\n",
      "            producer_node_query {\n",
      "              id: \"Transform\"\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"data_centric_pipeline\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"pipeline_run\"\n",
      "              }\n",
      "              name {\n",
      "                runtime_parameter {\n",
      "                  name: \"pipeline_run_id\"\n",
      "                  type: STRING\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            context_queries {\n",
      "              type {\n",
      "                name: \"node\"\n",
      "              }\n",
      "              name {\n",
      "                field_value {\n",
      "                  string_value: \"data_centric_pipeline.Transform\"\n",
      "                }\n",
      "              }\n",
      "            }\n",
      "            artifact_query {\n",
      "              type {\n",
      "                name: \"TransformGraph\"\n",
      "              }\n",
      "            }\n",
      "            output_key: \"transform_graph\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    outputs {\n",
      "      outputs {\n",
      "        key: \"model\"\n",
      "        value {\n",
      "          artifact_spec {\n",
      "            type {\n",
      "              name: \"Model\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      outputs {\n",
      "        key: \"model_run\"\n",
      "        value {\n",
      "          artifact_spec {\n",
      "            type {\n",
      "              name: \"ModelRun\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    parameters {\n",
      "      parameters {\n",
      "        key: \"custom_config\"\n",
      "        value {\n",
      "          field_value {\n",
      "            string_value: \"null\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      parameters {\n",
      "        key: \"eval_args\"\n",
      "        value {\n",
      "          field_value {\n",
      "            string_value: \"{\\n  \\\"num_steps\\\": 50\\n}\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      parameters {\n",
      "        key: \"run_fn\"\n",
      "        value {\n",
      "          field_value {\n",
      "            string_value: \"models.keras.model.run_fn\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "      parameters {\n",
      "        key: \"train_args\"\n",
      "        value {\n",
      "          field_value {\n",
      "            string_value: \"{\\n  \\\"num_steps\\\": 100\\n}\"\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "    upstream_nodes: \"SchemaGen\"\n",
      "    upstream_nodes: \"Transform\"\n",
      "    execution_options {\n",
      "      caching_options {\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "runtime_spec {\n",
      "  pipeline_root {\n",
      "    runtime_parameter {\n",
      "      name: \"pipeline_root\"\n",
      "      type: STRING\n",
      "      default_value {\n",
      "        string_value: \"gs://astute-pride-317802-kubeflowpipelines-default/tfx_pipeline_output/data_centric_pipeline\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  pipeline_run_id {\n",
      "    runtime_parameter {\n",
      "      name: \"pipeline_run_id\"\n",
      "      type: STRING\n",
      "    }\n",
      "  }\n",
      "}\n",
      "execution_mode: SYNC\n",
      "deployment_config {\n",
      "  type_url: \"type.googleapis.com/tfx.orchestration.IntermediateDeploymentConfig\"\n",
      "  value: \"\\n\\234\\001\\n\\020ExampleValidator\\022\\207\\001\\nOtype.googleapis.com/tfx.orchestration.executable_spec.PythonClassExecutableSpec\\0224\\n2tfx.components.example_validator.executor.Executor\\n\\220\\001\\n\\007Trainer\\022\\204\\001\\nOtype.googleapis.com/tfx.orchestration.executable_spec.PythonClassExecutableSpec\\0221\\n/tfx.components.trainer.executor.GenericExecutor\\n\\207\\001\\n\\tTransform\\022z\\nHtype.googleapis.com/tfx.orchestration.executable_spec.BeamExecutableSpec\\022.\\n,\\n*tfx.components.transform.executor.Executor\\n\\244\\001\\n\\020ImportExampleGen\\022\\217\\001\\nHtype.googleapis.com/tfx.orchestration.executable_spec.BeamExecutableSpec\\022C\\nA\\n?tfx.components.example_gen.import_example_gen.executor.Executor\\n\\216\\001\\n\\tSchemaGen\\022\\200\\001\\nOtype.googleapis.com/tfx.orchestration.executable_spec.PythonClassExecutableSpec\\022-\\n+tfx.components.schema_gen.executor.Executor\\n\\220\\001\\n\\rStatisticsGen\\022\\177\\nHtype.googleapis.com/tfx.orchestration.executable_spec.BeamExecutableSpec\\0223\\n1\\n/tfx.components.statistics_gen.executor.Executor\\022\\233\\001\\n\\020ImportExampleGen\\022\\206\\001\\nOtype.googleapis.com/tfx.orchestration.executable_spec.PythonClassExecutableSpec\\0223\\n1tfx.components.example_gen.driver.FileBasedDriver\"\n",
      "}\n",
      "\n",
      "INFO:absl:Adding upstream dependencies for component importexamplegen\n",
      "INFO:absl:Adding upstream dependencies for component latest-blessed-model-resolver\n",
      "INFO:absl:Adding upstream dependencies for component statisticsgen\n",
      "INFO:absl:   ->  Component: importexamplegen\n",
      "INFO:absl:Adding upstream dependencies for component schemagen\n",
      "INFO:absl:   ->  Component: statisticsgen\n",
      "INFO:absl:Adding upstream dependencies for component examplevalidator\n",
      "INFO:absl:   ->  Component: statisticsgen\n",
      "INFO:absl:   ->  Component: schemagen\n",
      "INFO:absl:Adding upstream dependencies for component transform\n",
      "INFO:absl:   ->  Component: importexamplegen\n",
      "INFO:absl:   ->  Component: schemagen\n",
      "INFO:absl:Adding upstream dependencies for component trainer\n",
      "INFO:absl:   ->  Component: schemagen\n",
      "INFO:absl:   ->  Component: transform\n",
      "Please access the pipeline detail page at http://314cd0b9ca67984e-dot-asia-east1.pipelines.googleusercontent.com/#/pipelines/details/486abe31-ff87-43b1-b8db-7c477e73f4ad\n",
      "Pipeline \"data_centric_pipeline\" updated successfully.\n",
      "\u001b[0m2021-06-26 04:02:05.942103: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
      "WARNING:absl:RuntimeParameter is only supported on Cloud-based DAG runner currently.\n",
      "CLI\n",
      "Creating a run for pipeline: data_centric_pipeline\n",
      "Detected Kubeflow.\n",
      "Use --engine flag if you intend to use a different orchestrator.\n",
      "/opt/conda/lib/python3.7/site-packages/kfp/_client.py:182: UserWarning: The host 314cd0b9ca67984e-dot-asia-east1.pipelines.googleusercontent.com does not contain the \"http\" or \"https\" protocol. Defaults to \"https\".\n",
      "  ' Defaults to \"https\".' % host)\n",
      "Run created for pipeline: data_centric_pipeline\n",
      "+=======================+======================================+========+===========================+============================================================================================================================+\n",
      "| pipeline_name         | run_id                               | status | created_at                | link                                                                                                                       |\n",
      "+=======================+======================================+========+===========================+============================================================================================================================+\n",
      "| data_centric_pipeline | 9a2a9cfc-a1a6-4498-b72b-247e0e215da1 | None   | 2021-06-26T04:02:11+00:00 | http://314cd0b9ca67984e-dot-asia-east1.pipelines.googleusercontent.com/#/runs/details/9a2a9cfc-a1a6-4498-b72b-247e0e215da1 |\n",
      "+=======================+======================================+========+===========================+============================================================================================================================+\n",
      "\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Update the pipeline\n",
    "!tfx pipeline update \\\n",
    "--pipeline-path=kubeflow_runner.py \\\n",
    "--endpoint={ENDPOINT}\n",
    "# You can run the pipeline the same way.\n",
    "!tfx run create --pipeline-name {PIPELINE_NAME} --endpoint={ENDPOINT}"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "template.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "name": "tf2-gpu.2-1.m73",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m73"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
